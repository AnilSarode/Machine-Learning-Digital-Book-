{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content:\n",
    "\n",
    "* [1. Introduction.](#1).\n",
    "* [1.1. The Learning Problem](#1.1)\n",
    "* [1.2. The Target Function ](#1.2)\n",
    "* [1.3. The Hypothesis](#1.3)\n",
    "* [1.4.The Loss Function ](#1.4)\n",
    "* [1.5. Is the Learning Problem Solvable?](#1.5)\n",
    "* [1.6. Is the Learning Problem Solvable?](#1.5)\n",
    "* [ 2.Independently, and Identically Distributed](#2)\n",
    "* [2.1.The Law of Large Numbers ](#2.1)\n",
    "* [2.2.Hoeffding’s inequality.](#2.2)\n",
    "* [2.3. Generalization Bound: 1st Attempt](#2.3)\n",
    "* [2.4.Examining the Independence Assumption.](#2.4)\n",
    "* [2.5.The Symmetrization Lemma](#2.5)\n",
    "* [ 3.The Growth Function](#3)\n",
    "* [3.1 The VC-Dimension](#3.1)\n",
    "* [ 3.2.The VC Generalization Bound](#3.2)\n",
    "* [3.3. Distribution-Based Bounds ](#3.3)\n",
    "* [References.](#r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gratitude\n",
    "\n",
    "Before going ahead I would like thanks to the Author **Mostafa Samir** whose blogs content I took to write this Chapter/Blog. To write this I have read and used some references which are included in the [References.](#r) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction.<a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "This chapter we will try to clear some fundamental idea how the machine learning problem is evolved by diving deep into mathematics but not that deep,but yes up to the level where we get the intuition.While going through the theory of this we touch at last the VC-dimension and Perceptron learning related to SVM and its limitation.Definitely in the chapter on neural network we will learn more about the Perceptron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. The Learning Problem.<a class=\"anchor\" id=\"1.1\"></a>\n",
    "\n",
    "By considering the supervised learning problem in which we have a dataset of observations $ S = \\{(x^{(i)}, y^{(i)}), …, (x^{(m)}, y^{(m)})\\}$ where $x^{(i)}$ is feature vector and $y^{(i)}$ is label and we wish to learn the how to get the value of $y^{(i)}$ given $x^{(i)}$. Note that each feature vector has $n$ features so the actual notation which we are almost following in each chapter is $x^{(i)}_{j}$, where $ i \\in (1 \\ldots m) $ i.e. number of training data(feature vector) and $j \\in (j \\ldots n)$ is number of features for each training data(feature vector). \n",
    "\n",
    "For an example, $x^{(i)}$ can be a vector of specific medical measurements and tests results (such as blood glucose level, and body mass index) of a patient and $y^{(i)}$ is the whether that patient is diabetic, and we wish to learn how to diagnose diabetes given the set of medical tests results.\n",
    "\n",
    "Lets consider the three points.\n",
    "\n",
    "1. We know that the values of $(x^{(i)},y^{(i)})$ in the dataset are just a random sample from a bigger population. In the concrete example we gave, the dataset is a random sample of a larger population of possible patients.We can formalize this fact by saying that the values of $x^{(i)}$ and $y^{(i)}$ are realizations of two random variables $X$ and $Y$ with probability distributions $P_X$ and $P_Y$ respectively.\n",
    "\n",
    "\n",
    "2. We know that there are some rules on the values of $X$ and $Y$ that we expect any realization of them to follow. In the diabetes example, we know that a value of a blood glucose test (a component of the $x$ vector) cannot be negative, so it belongs to a space of positive numbers. We also know that value of the label can either be $0$ (non-diabetic) or $1$ (diabetic), so it belongs to a space containing only $0$ and $1$. These kind of rules define what we formally call a space. We say that $X$ takes values drawn from the **input space $\\mathcal X $**, and **$Y$** from the **output space $\\mathcal Y$**.\n",
    " \n",
    " \n",
    "3. We know that there is a relation between the features and the labels, in a sense that the value of the features somehow determines the value of the label, or that the value of $Y$ is **conditioned** on the value of $X$. Formally, we express that by saying that there's a conditional probability $P(Y \\mid X)$. We can utilize this to compress the two distributions we had in the $1^{st}$ point into a single [joint distribution](https://github.com/AnilSarode/Machine-Learning-Digital-Book-/blob/master/Probability%20Theory%201/Probability%20Theory%201.ipynb) $P(X,Y)=P(X)P(Y\\mid X)$.\n",
    "\n",
    "\n",
    "With these three points, we can define a [statistical model](https://en.wikipedia.org/wiki/Statistical_model),that formalizes everything we know about the learning problem. \n",
    "\n",
    "\n",
    "## 1.2. The Target Function <a class=\"anchor\" id=\"1.2\"></a>\n",
    "\n",
    "\n",
    "It’s now clear from the statistical model we developed that the fundamental task of the machine learning process is to understand the nature of the conditional distribution $P(Y|X)$. To make our lives easier, we’ll avoid the hassle of working directly with the conditional distribution and instead introduce some kind of proxy that will be simpler to work with.\n",
    "\n",
    "There are two fundamental statistics we can use to decompose a random variable: these are the mean (or the expected value) and the variance. The mean is the value around which the random variable is centered, and the variance is the measure of how the random variable is distributed around the mean. Given two random variables $V$ and $W$, we can say that:\n",
    "\n",
    "$$\n",
    "V = \\mathbb{E}[V|W] + (V - \\mathbb{E}[V|W])\n",
    "$$\n",
    "\n",
    "where $\\mathbb{E}[V|W]$ is the conditional mean of the random variable $V$ given $W$. This essentially means that we can decompose the value of $V$ into two parts: the first can be explained in terms of the other variable $W$, and another noisy part that cannot be explained by $W$.here the explained by and not explained by means dependence and independence respectively.\n",
    "\n",
    "We can denote the unexplained part as an independent random variable $Z = V - \\mathbb{E}[V|W]$. It easy to see (using the [law of total expectation](https://en.wikipedia.org/wiki/Law_of_total_expectation)) that the mean of the $Z$ is zero. Hence $Z$ is a source of pure variance, that is the variance in V that cannot be explained by $W$.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[Z]=\\mathbb {E}[{[V]-\\mathbb {E}[V|W]}]= \\mathbb {E}[V]-\\mathbb {E}[\\mathbb {E}[V|W]]=\\mathbb {E}[V]-\\mathbb {E}[V]=0\n",
    "$$\n",
    "\n",
    "Now we can write the relationship between any two associated realizations $(w^{(i)},v^{(i)})$ of $W$ and $V$ as:\n",
    "\n",
    "$$\n",
    "v^{(i)} = \\mathbb{E}[V|W=w^{(i)}] + \\zeta\n",
    "$$\n",
    "\n",
    "where $\\zeta$ is a realization of the noise variable $Z$, we call that the **noise** term. We can apply the same reasoning on the our statistical model to get the following for any realization $(x^{(i)},y^{(i)})$\n",
    "\n",
    "$$\n",
    "y^{(i)} = \\mathbb{E}[Y|X=x^{(i)}] + \\zeta\n",
    "$$\n",
    "\n",
    "Assume there is some function $f(x)$ such that\n",
    "$$\n",
    "\\mathbb{E}[Y|X=x^{(i)}]=f(x)\n",
    "$$\n",
    "\n",
    "Which a function such that  $f:\\mathcal{X} \\rightarrow \\mathcal{Y} $\n",
    "\n",
    "That is, the conditional expectation is a function of the realizations $x$ that maps the input space $\\mathcal X$ to the output space $\\mathcal Y$. Now we can describe the relation between the features and the labels using the formula:\n",
    "$$\n",
    "y = f(x) + \\zeta\n",
    "$$\n",
    "\n",
    "Thus abstracting any mention of the conditional distribution $P(Y|X)$. Now we can use the function $f=f(x)$, which we call the **target function**, as the proxy for the conditional distribution.\n",
    "\n",
    "It’s called the target function because now onwards the machine learning process will work around this function. The machine learning process is now simplified to the task of estimating the function $f$. We'll later see how by this simplification we revealed the first source of error in our eventual solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. The Hypothesis<a class=\"anchor\" id=\"1.3\"></a>\n",
    "\n",
    "In the process to estimate the target function from the sample dataset, because we cannot investigate every single function that could exist in the universe (there is an infinite kinds of them), we attempt to make a hypothesis about the form of $f$. We can hypothesize that $f$ is a linear function of the input, or a cubic one, or some sophisticated non-linear function represented by a neural network. Whatever a form we hypothesize about the the target function $f$, it defines a space of possible functions we call the hypothesis space $\\mathcal H $.\n",
    "\n",
    "If we hypothesize that the function $f$ takes the form $ax+b$, then we're actually defining a hypothesis space $\\mathcal H$  where:\n",
    "\n",
    "$$\n",
    "\\mathcal{H} = \\{h:\\mathcal{X} \\rightarrow \\mathcal{Y} | h(x) = ax + b\\}\n",
    "$$\n",
    "\n",
    "That is the set of all functions $h$ mapping the input space to the output space, taking the form $ax+b$. A function $h_1(x)=3.2x−1$ is a concrete instance of $\\mathcal H$.The task of the machine learning process now is to pick from  $\\mathcal H$. a single concrete function h that best estimates the target function $f$. But how can measure how well a hypothesis function estimates the target? What is the criterion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.The Loss Function <a class=\"anchor\" id=\"1.4\"></a>\n",
    "\n",
    "One way of evaluating how well a hypothesis function is estimating the target function is by noticing that miss-labeling by the hypothesis should be discouraged, we obviously don't want our hypothesis function to make too many mistakes! This is the role of the loss function $L=L(y,\\hat{y} )$ (also called the **cost function** we saw first time in [Simple Linear Regression](https://github.com/AnilSarode/Machine-Learning-Digital-Book-/blob/master/Simple%20Linear%20Regression/Simple%20Linear%20Regression%20.ipynb)).\n",
    "\n",
    "The loss function takes the true label $y$ of some feature vector $x$, and the estimated label by our hypothesis $\\hat{y} =h(x)$, then it examines how far our estimate is the from the true value and reports how much do we loose by using that hypothesis.\n",
    "\n",
    "Using the loss function, we can calculate the performance of a hypothesis function $h$ on the entire dataset by taking the mean of the losses on each sample. We call this quantity the **in-sample error**, or as we'll call it from now on: the **empirical risk**:\n",
    "\n",
    "$$\n",
    "R_{\\text{emp}}(h) = \\frac{1}{m}\\sum_{i=1}^{m}L(y^{(i)}, h(x^{(i)}))\n",
    "$$\n",
    "\n",
    "It's empirical because we calculate it form the empirical data we sampled in the dataset, but why don't we call an empirical error? The reason behind that is notational; if we used the term error we'll end up using $E$ to refer to it in the math, and this could lead into confusion with the notation for the expected value $\\mathcal{E}$, hence we use risk and $R$ instead. The notational choice is also justified by the fact that error and risk are semantically close.\n",
    "\n",
    "By defining the empirical error, the machine learning process can now chose the hypothesis with the last $R_{emp}(h)$ as the best estimation of the target function $f$. But is that it? Can we declare the problem solved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.The Generalization Error<a class=\"anchor\" id=\"1.5\"></a>\n",
    "\n",
    "What do you think of the following above a as a hypothesis? We memorize each sample in the dataset in a table, and whenever the the hypothesis function is queried with a feature vector $x^{(i)}$ we respond with associated $y^{(i)}$ in the table. Obviously, $R_{emp}=0$ for this hypothesis. It is also obvious that this is a lousy solution to the learning problem!\n",
    "\n",
    "Remember that the goal is to learn **the probability distribution underlying the dataset**, not just do well on the dataset samples. That means that the hypothesis should also have low errors on new unseen data samples from the distribution. This is obviously not true for the proposed memorization hypothesis.\n",
    "\n",
    "For the hypothesis to perform well on unseen new data is for the hypothesis to generalize over the underlying probability distribution. We formally capture that by defining the generalization error (also referred to as the **risk**), it simply the expected value of the loss over the whole joint distribution $P(X,Y):$\n",
    "\n",
    "$$\n",
    "R(h) = \\mathbb{E}_{(x,y) \\sim P(X,Y)}[L(y, h(x))]\n",
    "$$\n",
    "\n",
    "Now we can say that the solution to the learning problem is the hypothesis with the least generalization error $R$. Simple, isn’t it? Well, here is the catch: we cannot calculate $R$ because we do not know the joint distribution $P(X,Y)!$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Is the Learning Problem Solvable? <a class=\"anchor\" id=\"1.6\"></a>\n",
    "\n",
    "Now that's bad , we're stuck our **empirical risk minimization (ERM)** strategy, is there even a chance that we can solve the learning problem?!\n",
    "\n",
    "Well, we shouldn't lose hope so fast. There is a chance to solve the learning problem if there's a way that we can make both $R_{emp}(h)$ and $R(h)$ close to each other, but is that possible?\n",
    "\n",
    "This question boils down to calculating the following probability:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}[\\sup_{h \\in \\mathcal{H}}|R(h) - R_{\\text{emp}}(h)| > \\epsilon]\n",
    "$$\n",
    "\n",
    "That is the probability that the least upper bound (that is the [supremum](https://en.wikipedia.org/wiki/Infimum_and_supremum) $\\sup_{h \\in \\mathcal{H}}$) of the absolute difference between $R$ and $R_{emp}$ is greater than a very small value $\\epsilon$. If this probability is sufficiently small, that is there is a very little chance that $R_{emp}$ differs much than $R$, then the learning problem is solvable.\n",
    "\n",
    "here for every $h$ from $\\mathcal H$ we estimate the probability of absolute difference between $R$ and $R_{emp}$\n",
    "grater than $\\epsilon$ (which is a small value).We want this probability to be as small as possible which means the absolute difference between $R$ and $R_{emp}$ should not be big not even greater than small value  $\\epsilon$.\n",
    "\n",
    "\n",
    "Here we concluded by noticing that minimizing the empirical risk (or the training error) is not in itself a solution to the learning problem, it could only be considered a solution if we can guarantee that the difference between the training error and the generalization error (which is also called the generalization gap) is small enough. We formalized such requirement using the probability:\n",
    "$$\n",
    "\\mathbb{P}\\left[\\sup_{h \\in \\mathcal{H}} |R(h) - R_{\\text{emp}(h)}| > \\epsilon\\right]\n",
    "$$\n",
    "\n",
    "That is if this probability is small, we can guarantee that the difference between the errors is not much, and hence the learning problem can be solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Independently, and Identically Distributed. <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "The world can be a very messy place! This is a problem that faces any theoretical analysis of a real world phenomenon; because usually we can’t really capture all the messiness in mathematical terms, and even if we’re able to; we usually don’t have the tools to get any results from such a messy mathematical model.\n",
    "\n",
    "So in order for theoretical analysis to move forward, some assumptions must be made to simplify the situation at hand, we can then use the theoretical results from that simplification to infer about reality.\n",
    "\n",
    "Assumptions are common practice in theoretical work. Assumptions are not bad in themselves, only bad assumptions are bad! As long as our assumptions are reasonable and not crazy, they’ll hold significant truth about reality.\n",
    "\n",
    "A reasonable assumption we can make about the problem we have at hand is that our training dataset samples are independently, and identically distributed (or i.i.d. for short), that means that all the samples are drawn from the same probability distribution and that each sample is independent from the others.\n",
    "\n",
    "This assumption is essential for us. We need it to start using the tools from probability theory to investigate our generalization probability, and it’s a very reasonable assumption because:\n",
    "\n",
    "1. It’s more likely for a dataset used for inferring about an underlying probability distribution to be all sampled for that same distribution. If this is not the case, then the statistics we get from the dataset will be noisy and won’t correctly reflect the target underlying distribution.\n",
    "\n",
    "\n",
    "2. It’s more likely that each sample in the dataset is chosen without considering any other sample that has been chosen before or will be chosen after. If that’s not the case and the samples are dependent, then the dataset will suffer from a bias towards a specific direction in the distribution, and hence will fail to reflect the underlying distribution correctly.\n",
    "So we can build upon that assumption with no fear\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.The Law of Large Numbers <a class=\"anchor\" id=\"2.1\"></a>\n",
    "\n",
    "Most of us, since we were kids, know that if we tossed a fair coin a large number of times, roughly half of the times we're gonna get heads. This is an instance of wildly known fact about probability that if we repeat an experiment for a sufficiency large amount of times, the average outcome of these experiments (or, more formally, the sample mean) will be very close to the true mean of the underlying distribution. This fact is formally captured into what we call [The law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers):\n",
    "\n",
    "If $x_1, x_2, …, x_m$  are $m$ i.i.d. samples of a random variable $X$ distributed by $P$. then for a small positive non-zero value $\\epsilon$:\n",
    "\n",
    "$$\n",
    "\\lim_{m \\rightarrow \\infty} \\mathbb{P}\\left[\\left|\\mathop{\\mathbb{E}}_{X \\sim P}[X] - \\frac{1}{m}\\sum_{i=1}^{m}x_i \\right| > \\epsilon\\right] = 0  \\ldots (2.1)\n",
    "$$\n",
    "\n",
    "This version of the law is called the weak law of large numbers. It's weak because it guarantees that as the sample size goes larger, the sample and true means will likely be very close to each other by a non-zero distance no greater than epsilon. On the other hand, the strong version says that with very large sample size, the sample mean is almost surely equal to the true mean.\n",
    "\n",
    "The formulation of the weak law lends itself naturally to use with our generalization probability. By recalling that the **empirical risk is actually the sample mean of the errors** and the **risk is the true mean**, for a single hypothesis $h$ we can say that:\n",
    "\n",
    "$$\n",
    "\\lim_{m \\rightarrow \\infty} \\mathbb{P}\\left[\\left|R(h) - R_{\\text{emp}}(h)\\right| > \\epsilon \\right] = 0 \\ldots (2.2)\n",
    "$$\n",
    "\n",
    "Well, that’s a progress, A pretty small one, but still a progress! Can we do any better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.Hoeffding’s inequality. <a class=\"anchor\" id=\"2.2\"></a>\n",
    "\n",
    "The law of large numbers is like someone pointing the directions to you when you’re lost, they tell you that by following that road you’ll eventually reach your destination, but they provide no information about how fast you’re gonna reach your destination, what is the most convenient vehicle, should you walk or take a cab, and so on.\n",
    "\n",
    "To our destination of ensuring that the training and generalization errors do not differ much, we need to know more info about the how the road down the law of large numbers look like. These info are provided by what we call the [concentration inequalities](https://en.wikipedia.org/wiki/Concentration_inequality). This is a set of inequalities that quantifies how much random variables (or function of them) deviate from their expected values (or, also, functions of them). One inequality of those is Heoffding’s inequality:\n",
    "\n",
    "If $x_1, x_2, …, x_m$  are $m$ i.i.d. samples of a random variable $X$ distributed by $P$, and $a\\leq xi\\leq b$ for every $i$, then for a small positive non-zero value $\\epsilon$:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left[\\left|\\mathop{\\mathbb{E}}_{X \\sim P}[X] - \\frac{1}{m}\\sum_{i=0}^{m}x_i\\right| > \\epsilon\\right] \\leq 2\\exp\\left(\\frac{-2m\\epsilon^2}{(b -a)^2}\\right)  \\ldots (2.3)\n",
    "$$\n",
    "\n",
    "You probably see why we specifically chose Heoffding's inequality from among the others. We can naturally apply this inequality to our generalization probability, assuming that our errors i.e. empirical risk and risk  are bounded between $0$ and $1$ (which is a reasonable assumption, as we can get that using a $0/1$ loss function or by squashing any other loss between 0 and 1) and get for a single hypothesis $h$:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}[|R(h) - R_{\\text{emp}}(h)| > \\epsilon] \\leq 2\\exp(-2m\\epsilon^2) \\ldots (2.4)\n",
    "$$\n",
    "\n",
    "This means that the probability of the difference between the training and the generalization errors exceeding $\\epsilon$ exponentially decays as the dataset size goes larger. This should align well with our practical experience that **the bigger the dataset gets, the better the results become**.\n",
    "\n",
    "If you noticed, all our analysis up till now in second section  was focusing on a single hypothesis $h$. But the learning problem doesn't know that single hypothesis beforehand, it needs to pick **one** out of an entire hypothesis space $\\mathcal H$, so we need a generalization bound that reflects the challenge of choosing the right hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Generalization Bound: 1st Attempt<a class=\"anchor\" id=\"2.3\"></a>\n",
    "\n",
    "In order for the entire hypothesis space to have a generalization gap greater than $\\epsilon$, at least one of its hypothesis: $h_1$ or $h_2$ or $h_3$ or $\\ldots$ etc should have. This can be expressed formally by stating that\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left[\\sup_{h \\in \\mathcal{H}}|R(h) - R_\\text{emp}(h)| > \\epsilon\\right] = \\mathbb{P}\\left[\\bigcup_{h \\in \\mathcal{H}} |R(h) - R_\\text{emp}(h)| > \\epsilon\\right]  \\ldots (2.5)\n",
    "$$\n",
    "\n",
    "Where $\\bigcup $ denotes the union of the events, which also corresponds to the [logical **OR** operator](https://en.wikipedia.org/wiki/Logical_disjunction#Union). Using the [union bound inequality](https://en.wikipedia.org/wiki/Boole%27s_inequality), we get:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left[\\sup_{h \\in \\mathcal{H}}|R(h) - R_\\text{emp}(h)| > \\epsilon\\right] \\leq \\sum_{h \\in \\mathcal{H}} \\mathbb{P}[|R(h) - R_\\text{emp}(h)| > \\epsilon] \\ldots (2.6)\n",
    "$$\n",
    "\n",
    "We exactly know the bound on the probability under the summation from our analysis using the Heoffding’s inequality i.e. using the equation $(2.4)$and $(2.6)$, so we end up with:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left[\\sup_{h \\in \\mathcal{H}}|R(h) - R_\\text{emp}(h)| > \\epsilon\\right] \\leq 2|\\mathcal{H}|\\exp(-2m\\epsilon^2) \\ldots (2.7)\n",
    "$$\n",
    "\n",
    "Where $\\mid \\mathcal H \\mid $ is the size of the hypothesis space. By denoting the right hand side of the above inequality by $\\delta$, we can say that with a confidence $1−\\delta$:\n",
    "\n",
    "$$\n",
    "|R(h) - R_\\text{emp}(h)| \\leq \\epsilon \\Rightarrow R(h) \\leq R_\\text{emp}(h) + \\epsilon \\ldots (2.8)\n",
    "$$\n",
    "\n",
    "And with some basic algebra, we can express $\\epsilon$ in terms of $\\delta$ and get:\n",
    "\n",
    "say\n",
    "\n",
    "$$\n",
    "2|\\mathcal{H}|\\exp(-2m\\epsilon^2)= \\delta\n",
    "$$\n",
    "\n",
    "Taking natural log on the  both side\n",
    "\n",
    "$$\n",
    " \\ln [|\\mathcal{H}|\\exp(-2m\\epsilon^2)]=\\ln{\\frac{\\delta}{2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ln |\\mathcal{H}|-2m\\epsilon^2=\\ln{\\frac{\\delta}{2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\epsilon =\\sqrt{\\frac{\\ln|\\mathcal{H}| + \\ln{\\frac{2}{\\delta}}}{2m}}\n",
    "$$\n",
    "\n",
    "from equation $(2.8)$\n",
    "$$\n",
    "R(h) \\leq R_\\text{emp}(h) + \\sqrt{\\frac{\\ln|\\mathcal{H}| + \\ln{\\frac{2}{\\delta}}}{2m}}\\ldots (2.9)\n",
    "$$\n",
    "\n",
    "This is our first **generalization bound**, it states that the generalization error is bounded by the training error(Risk) plus a function of the hypothesis space size $|\\mathcal{H}|$ and the dataset size $m$. We can also see that the the larger the hypothesis space gets, the larger the generalization error becomes. This explains why the memorization hypothesis form last time, which theoretically has $|\\mathcal{H}| =\\infty$, fails miserably as a solution to the learning problem despite having $R_{emp}=0$; because for the memorization hypothesis $h_{mem}$:\n",
    "\n",
    "$$\n",
    "R(h_\\text{mem}) \\leq 0 + \\infty \\leq \\infty\n",
    "$$\n",
    "\n",
    "But wait a second! For a linear hypothesis of the form $h(x)=wx+b$, we also have $|\\mathcal{H}| =\\infty$ as there is infinitely many lines that can be drawn. So the generalization error of the linear hypothesis space should be unbounded just as the memorization hypothesis! **If that's true, why does perceptrons, logistic regression, support vector machines and essentially any ML model that uses a linear hypothesis work?**\n",
    "\n",
    "Our theoretical result was able to account for some phenomena (the memorization hypothesis, and any finite hypothesis space) but not for others (the linear hypothesis, or other infinite hypothesis spaces that empirically work). This means that there’s still something missing from our theoretical model, and it’s time for us to revise our steps. A good starting point is from the source of the problem itself, which is the infinity in $|\\mathcal{H}| $.\n",
    "\n",
    "Notice that the term $|\\mathcal{H}|$ resulted from our use of the union bound. The basic idea of the union bound is that it bounds the probability by the worst case possible, which is when all the events under union are mutually independent. This bound gets more tight as the events under consideration get less dependent. In our case, for the bound to be tight and reasonable, we need the following to be true:\n",
    "\n",
    "For every two hypothesis $h_1, h_2 \\in \\mathcal{H}$ the two events $|R(h_1) - R_\\text{emp}(h_1)| > \\epsilon$ are likely to be independent. This means that the event that $h_{1}$ has a generalization gap bigger than $\\epsilon$ should be independent of the event that also $h_2$ has a generalization gap bigger than $\\epsilon$, no matter how much $h_1$ and $h_2$ are close or related; the events should be coincidental.\n",
    "\n",
    "But is that true?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.Examining the Independence Assumption.<a class=\"anchor\" id=\"2.4\"></a>\n",
    "\n",
    "The first question we need to ask here is why do we need to consider every possible hypothesis in  $\\mathcal H$? This may seem like a trivial question; as the answer is simply that because the learning algorithm can search the entire hypothesis space looking for its optimal solution. While this answer is correct, we need a more formal answer in light of the generalization inequality we're studying.\n",
    "\n",
    "The formulation of the generalization inequality reveals a main reason why we need to consider all the hypothesis in $\\mathcal H$. It has to do with the existence of $\\sup_{h \\in \\mathcal{H}}$. **The supremum in the inequality guarantees that there's a very little chance that the biggest generalization gap possible is greater than $\\epsilon$;** this is a strong claim and if we omit a single hypothesis out of  $\\mathcal H$ , we might miss that “biggest generalization gap possible” and lose that strength, and that’s something we cannot afford to lose. We need to be able to make that claim to ensure that the learning algorithm would never land on a hypothesis with a bigger generalization gap than $\\epsilon$.\n",
    "\n",
    "![](fig.png)\n",
    "\n",
    "$$\\mathbf{Figure 1}$$\n",
    "\n",
    "Looking at the above(Figure 1) plot of binary classification problem, it’s clear that this rainbow of hypothesis produces the same classification on the data points, so all of them have the same empirical risk. So one might think, as they all have the same $R_{emp}$, why not choose one and omit the others?!\n",
    "\n",
    "This would be a very good solution if we’re only interested in the empirical risk, but our inequality takes into its consideration the out-of-sample risk as well, which is expressed as:\n",
    "\n",
    "$$\n",
    "R(h) = \\mathop{\\mathbb{E}}_{(x,y) \\sim P}[L(y, h(x))] = \\int_{\\mathcal{Y}}\\int_{\\mathcal{X}}L(y, h(x))P(x, y)\\,\\mathrm{d}x \\,\\mathrm{d}y\n",
    "$$\n",
    "\n",
    "This is an integration over every possible combination of the whole input and output spaces $\\mathcal{X, Y}$. So in order to ensure our supremum claim, we need the hypothesis to cover the whole of $\\mathcal{X \\times Y}$, hence we need all the possible hypotheses in $\\mathcal{H}$.\n",
    "\n",
    "Now that we've established that we do need to consider every single hypothesis in $\\mathcal{H}$, we can ask ourselves: **are the events of each hypothesis having a big generalization gap are likely to be independent?**\n",
    "\n",
    "Well, Not even close! Take for example the rainbow of hypotheses in the above plot, it's very clear that if the red hypothesis has a generalization gap greater than $\\epsilon$, then, with 100% certainty, every hypothesis with the same slope in the region above it will also have that. The same argument can be made for many different regions in the $\\mathcal{X \\times Y}$ space with different degrees of certainty as in the following Figure 2.\n",
    "\n",
    "![](fig1.png)\n",
    "\n",
    "$$\\mathbf{Figure 2}$$\n",
    "\n",
    "But this is not helpful for our mathematical analysis, as the regions seems to be dependent on the distribution of the sample points and there is no way we can precisely capture these dependencies mathematically, and we cannot make assumptions about them without risking to compromise the supremum claim.\n",
    "\n",
    "So the union bound and the independence assumption seem like the best approximation we can make,but it highly overestimates the probability and makes the bound very loose, and very pessimistic!\n",
    "\n",
    "However, what if somehow we can get a very good estimate of the risk $R(h)$ without needing to go over the whole of the $\\mathcal{X \\times Y}$ space, would there be any hope to get a better bound?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.The Symmetrization Lemma <a class=\"anchor\" id=\"2.5\"></a>\n",
    "\n",
    "\n",
    "Let’s think for a moment about something we do usually in machine learning practice. In order to measure the accuracy of our model, we hold out a part of the training set to evaluate the model on after training, and we consider the model’s accuracy on this left out portion as an estimate for the generalization error. This works because we assume that this **test set** is drawn i.i.d. from the same distribution of the training set (this is why we usually shuffle the whole dataset beforehand to break any correlation between the samples).\n",
    "\n",
    "It turns out that we can do a similar thing mathematically, but instead of taking out a portion of our dataset $S$, we imagine that we have another dataset $S'$ with also size $m$, we call this the **ghost dataset**. Note that this has no practical implications, we don't need to have another dataset at training, it's just a mathematical trick we're gonna use to git rid of the restrictions of $R(h)$ in the inequality.\n",
    "\n",
    "We're not gonna go over the [proof here](https://mlweb.loria.fr/book/en/symmetrization.html), but using that ghost dataset one can actually prove that\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left[\\sup_{h \\in \\mathcal{H}}\\left|R(h) - R_\\text{emp}(h)\\right| > \\epsilon\\right] \\leq 2\\mathbb{P}\\left[\\sup_{h \\in \\mathcal{H}}\\left|R_\\text{emp}(h) - R_\\text{emp}'(h)\\right| > \\frac{\\epsilon}{2}\\right] \\hspace{2em} \\ldots(2.10)\n",
    "$$\n",
    "\n",
    "where $R'_{emp}(h)$ is the empirical risk of hypothesis $h$ on the ghost dataset. This means that the probability of the largest generalization gap being bigger than $\\epsilon$ is at most twice the probability that the empirical risk difference between $S,S'$ is larger than $\\frac{\\epsilon}{2}$. Now that the right hand side in expressed only in terms of empirical risks, we can bound it without needing to consider the the whole of $\\mathcal{X \\times Y}$, and hence we can bound the term with the risk $R(h)$ without considering the whole of input and output spaces!\n",
    "\n",
    "This, which is called the symmetrization lemma, was one of the two key parts in the work of Vapnik-Chervonenkis (1971)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.The Growth Function<a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "Now that we are bounding only the empirical risk, if we have many hypotheses that have the same empirical risk (a.k.a. producing the same labels/values on the data points), we can safely choose one of them as a representative of the whole group, we'll call that an effective hypothesis, and discard all the others.\n",
    "\n",
    "By only choosing the distinct effective hypotheses on the dataset $S$, we restrict the hypothesis space $\\mathcal{H}$ to a smaller subspace that depends on the dataset $\\mathcal{H}_{|S}$.\n",
    "\n",
    "We can assume the independence of the hypotheses in $\\mathcal{H}_{|S}$ like we did before with $\\mathcal{H}_{|S}$ (but it's more plausible now), and use the union bound to get that:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left[\\sup_{h \\in \\mathcal{H}_{|S\\cup S'}}\\left|R_\\text{emp}(h) - R_\\text{emp}'(h)\\right|>\\frac{\\epsilon}{2}\\right] \\leq \\left|\\mathcal{H}_{|S\\cup S'}\\right| \\mathbb{P}\\left[\\left|R_\\text{emp}(h) - R_\\text{emp}'(h)\\right|>\\frac{\\epsilon}{2}\\right] \\ldots (3.1)\n",
    "$$\n",
    "\n",
    "Notice that the hypothesis space is restricted by $S \\cup S’$ because we using the empirical risk on both the original dataset $S$ and the ghost $S'$. **The question now is what is the maximum size of a restricted hypothesis space?** The answer is very simple; we consider a hypothesis to be a new effective one if it produces new labels/values on the dataset samples, then **the maximum number of distinct hypothesis (a.k.a the maximum number of the restricted space) is the maximum number of distinct labels/values the dataset points can take.** A cool feature about that maximum size is that its a combinatorial measure, so we don’t need to worry about how the samples are distributed!\n",
    "\n",
    "For simplicity, we'll focus now on the case of binary classification, in which $\\mathcal{Y}=\\{-1, +1\\}$. Later we'll show that the same concepts can be extended to both multi-class classification and regression. In that case, for a dataset with $m$ samples, each of which can take one of two labels: either $-1$ or $+1$, the maximum number of distinct labellings is $2^{m}$.\n",
    "\n",
    "We'll define the maximum number of distinct labellings/values on a dataset $S$ of size $m$ by a hypothesis space $\\mathcal H$ as the **growth function** of $\\mathcal H$ given $m$, and we'll denote that by $\\Delta_\\mathcal{H}(m)$. It's called the growth function because it's value for a single hypothesis space $\\mathcal H$  (aka the size of the restricted subspace $\\mathcal H_{|S}$ ) grows as the size of the dataset grows. Now we can say that:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left[\\sup_{h \\in \\mathcal{H}_{|S\\cup S'}}\\left|R_\\text{emp}(h) - R_\\text{emp}'(h)\\right|>\\frac{\\epsilon}{2}\\right] \\leq \\Delta_\\mathcal{H}(2m) \\mathbb{P}\\left[\\left|R_\\text{emp}(h) - R_\\text{emp}'(h)\\right|>\\frac{\\epsilon}{2}\\right] \\hspace{2em}\\ldots(3.2)\n",
    "$$\n",
    "\n",
    "For the binary classification case, we can say that:\n",
    "$$\n",
    "\\Delta_\\mathcal{H}(m) \\leq 2^m  \\ldots (3.3)\n",
    "$$\n",
    "\n",
    "But $2^{m}$ is exponential in $m$ and would grow too fast for large datasets, which makes the odds in our inequality go too bad too fast! Is that the best bound we can get on that growth function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The VC-Dimension <a class=\"anchor\" id=\"3.1\"></a>\n",
    "\n",
    "\n",
    "\n",
    "The $2^{m}$ bound is based on the fact that the hypothesis space $\\mathcal{H} $ can produce all the possible labellings on the $m$ data points. If a hypothesis space can indeed produce all the possible labels on a set of data points, we say that the hypothesis space **shatters** that set.\n",
    "\n",
    "But can any hypothesis space shatter any dataset of any size? Let's investigate that with the binary classification case and the $\\mathcal{H}$ of linear classifiers $sign(wx+b)$. The following plot shows how many ways a linear classifier in $2D$ can label 3 points (on the first plot ) and 4 points (on the second plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 10)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "x_axis=[2,4,5]\n",
    "y_axis=[3,5,2]\n",
    "\n",
    "fig, ((ax1,ax2),(ax3,ax4),(ax5,ax6),(ax7,ax8)) = plt.subplots(4,2,figsize=(10,8))\n",
    "\n",
    "ax1.plot(x_axis,y_axis,'ko',markersize=5)\n",
    "ax1.plot([0,7],[4,8],'r')\n",
    "ax1.legend([\"Class0\"], loc=\"upper left\",prop=dict(size=8))\n",
    "ax1.set_xlim([0,10])\n",
    "ax1.set_ylim([0,10])\n",
    "\n",
    "x_axis=[2,4]\n",
    "y_axis=[3,5]\n",
    "\n",
    "ax2.plot(x_axis,y_axis,'ko',markersize=5)\n",
    "ax2.plot(5,2,'o',markersize=5)\n",
    "ax2.plot([0,6],[0.5,3],'r')\n",
    "ax2.legend([\"Class0\", \"Class1\"], loc=\"upper left\",prop=dict(size=8))\n",
    "ax2.set_xlim([0,10])\n",
    "ax2.set_ylim([0,10])\n",
    "\n",
    "x_axis=[4,5]\n",
    "y_axis=[5,2]\n",
    "\n",
    "ax3.plot(x_axis,y_axis,'ko',markersize=5)\n",
    "ax3.plot(2,3,'o',markersize=5)\n",
    "ax3.plot([3,3.5],[0,8],'r')\n",
    "ax3.legend([\"Class0\", \"Class1\"], loc=\"upper left\",prop=dict(size=8))\n",
    "ax3.set_xlim([0,10])\n",
    "ax3.set_ylim([0,10])\n",
    "\n",
    "x_axis=[2,5]\n",
    "y_axis=[3,2]\n",
    "\n",
    "ax4.plot(x_axis,y_axis,'ko',markersize=5)\n",
    "ax4.plot(4,5,'o',markersize=5)\n",
    "ax4.plot([1.5,8],[5,2.5],'r')\n",
    "ax4.legend([\"Class0\", \"Class1\"], loc=\"upper left\",prop=dict(size=8))\n",
    "ax4.set_xlim([0,10])\n",
    "ax4.set_ylim([0,10])\n",
    "\n",
    "x_axis=[2,4,5]\n",
    "y_axis=[3,5,2]\n",
    "\n",
    "ax5.plot(x_axis,y_axis,'o',markersize=5)\n",
    "ax5.plot([5,7],[0,8],'r')\n",
    "ax5.legend([\"Class1\"], loc=\"upper left\",prop=dict(size=8))\n",
    "ax5.set_xlim([0,10])\n",
    "ax5.set_ylim([0,10])\n",
    "\n",
    "\n",
    "x_axis=[2,4]\n",
    "y_axis=[3,5]\n",
    "\n",
    "ax6.plot(x_axis,y_axis,'o',markersize=5)\n",
    "ax6.plot(5,2,'ko',markersize=5)\n",
    "ax6.plot([0,6],[0.2,5],'r')\n",
    "ax6.legend([\"Class1\", \"Class0\"], loc=\"upper left\",prop=dict(size=8))\n",
    "ax6.set_xlim([0,10])\n",
    "ax6.set_ylim([0,10])\n",
    "\n",
    "x_axis=[4,5]\n",
    "y_axis=[5,2]\n",
    "\n",
    "ax7.plot(x_axis,y_axis,'o',markersize=5)\n",
    "ax7.plot(2,3,'ko',markersize=5)\n",
    "ax7.plot([3.5,3.5],[0,8],'r')\n",
    "ax7.legend([\"Class1\", \"Class0\"], loc=\"upper left\",prop=dict(size=8))\n",
    "ax7.set_xlim([0,10])\n",
    "ax7.set_ylim([0,10])\n",
    "\n",
    "x_axis=[2,5]\n",
    "y_axis=[3,2]\n",
    "\n",
    "ax8.plot(x_axis,y_axis,'o',markersize=5)\n",
    "ax8.plot(4,5,'ko',markersize=5)\n",
    "ax8.plot([1.5,8],[5.5,2.5],'r')\n",
    "ax8.legend([\"Class1\", \"Class0\"], loc=\"upper left\",prop=dict(size=8))\n",
    "ax8.set_xlim([0,10])\n",
    "ax8.set_ylim([0,10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 10)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X=[2,4,6,8]\n",
    "Y=[3,6,2,5]\n",
    "\n",
    "fig, ((ax1,ax2)) = plt.subplots(1,2,figsize=(8,4))\n",
    "\n",
    "\n",
    "ax1.plot(X,Y,'ko',markersize=5)\n",
    "ax1.set_xlim([0,10])\n",
    "ax1.set_ylim([0,10])\n",
    "\n",
    "X=[2,8]\n",
    "Y=[3,5]\n",
    "W=[4,6]\n",
    "Z=[6,2]\n",
    "ax2.plot(X,Y,'ko',markersize=5)\n",
    "ax2.plot(W,Z,'o',markersize=5)\n",
    "ax2.plot([0,10],[5.9,2.5],'r')\n",
    "ax2.legend([\"Class0\", \"Class1\"], loc=\"upper left\",prop=dict(size=8))\n",
    "ax2.set_xlim([0,12])\n",
    "ax2.set_ylim([0,10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the the three points, the hypothesis shattered the set of points and produced all the possible $2^{3}=8$ labellings. However for the four points,the hypothesis couldn't get more than $14$ and never reached $2^{4}=16$, so it failed to shatter this set of points. Actually, no linear classifier in $2D$ can shatter any set of $4$ points, not just that set; **because there will always be two labellings that cannot be produced by a linear classifier which is depicted in the above second plot.**\n",
    "\n",
    "So it's possible for a hypothesis space $\\mathcal{H}$ to be unable to shatter all sizes. This fact can be used to get a better bound on the growth function, and this is done using Sauer's lemma:\n",
    " \n",
    "If a hypothesis space $\\mathcal{H}$ cannot shatter any dataset with size more than $k$, then:\n",
    "\n",
    "$$\n",
    "\\Delta_{\\mathcal{H}}(m) \\leq \\sum_{i=0}^{k}\\binom{m}{i} \\ldots (3.4)\n",
    "$$\n",
    "\n",
    "This was the other key part of Vapnik-Chervonenkis work (1971), but it's named after another mathematician, Norbert Sauer; because it was independently proved by him around the same time (1972). However, Vapnik and Chervonenkis weren’t completely left out from this contribution; as that $k$, which is the maximum number of points that can be shattered by $\\mathcal{H}$ , is now called the Vapnik-Chervonenkis-dimension or the **VC-dimension $d_{vc}$ of $\\mathcal{H}$**.\n",
    "\n",
    "For the case of the linear classifier in $2D$, $d_{vc}=3$. In general, it can be proved that hyperplane classifiers (the higher-dimensional generalization of line classifiers) in $\\mathbb{R}^n$ space has $d_{vc}=n+1$.\n",
    "\n",
    "The bound on the growth function provided by [sauer's lemma](https://mlweb.loria.fr/book/en/SauerShelah.html) is indeed much better than the exponential one we already have, it's actually polynomial! Using algebraic manipulation, we can prove that:$d_\\mathrm{vc} = n + 1$\n",
    "\n",
    "$$\n",
    "\\Delta_\\mathcal{H}(m) \\leq \\sum_{i=0}^{k}\\binom{m}{i} \\leq \\left(\\frac{me}{d_\\mathrm{vc}}\\right)^{d_\\mathrm{vc}}\\leq O(m^{d_\\mathrm{vc}})  \\ldots (3.5)\n",
    "$$\n",
    "\n",
    "Where $O$ refers to the Big-$O$ notation for functions asymptotic (near the limits) behavior, and $e$ is the mathematical constant.\n",
    "\n",
    "Thus we can use the VC-dimension as a proxy for growth function and, hence, for the size of the restricted space $\\mathcal{H}_{|S}$. In that case, $d_{vc}$ would be a measure of the complexity or richness of the hypothesis space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.The VC Generalization Bound <a class=\"anchor\" id=\"3.2\"></a>\n",
    "\n",
    "With a little change in the constants, it can be shown that Heoffding’s inequality is applicable on the probability $\\mathbb{P}\\left[|R_\\mathrm{emp}(h) - R_\\mathrm{emp}’(h)| > \\frac{\\epsilon}{2}\\right]$  With that, and by combining inequalities $(2.10)$ and $(3.2)$, or from [here](https://mlweb.loria.fr/book/en/VCbound.html) the **Vapnik-Chervonenkis theory follows**:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left[\\sup_{h \\in \\mathcal{H}}|R(h) - R_\\mathrm{emp}(h)| > \\epsilon\\right] \\leq 4\\Delta_\\mathcal{H}(2m)\\exp\\left(-\\frac{m\\epsilon^2}{8}\\right)\n",
    "$$\n",
    "\n",
    "This can be re-expressed as a bound on the generalization error, just as we did earlier with the previous bound, to get the **VC generalization bound**:\n",
    "\n",
    "$$\n",
    "R(h) \\leq R_\\mathrm{emp}(h) + \\sqrt{\\frac{8\\ln\\Delta_\\mathcal{H}(2m) + 8\\ln\\frac{4}{\\delta}}{m}}\n",
    "$$\n",
    "\n",
    "or, by using the bound on growth function in terms of $d_{vc}$ as:\n",
    "$$\n",
    "R(h) \\leq R_\\mathrm{emp}(h) + \\sqrt{\\frac{8d_\\mathrm{vc}(\\ln\\frac{2m}{d_\\mathrm{vc}} + 1) + 8\\ln\\frac{4}{\\delta}}{m}}\n",
    "$$\n",
    "\n",
    "This is a significant result! It’s a clear and concise mathematical statement that the learning problem is solvable, and that for infinite hypotheses spaces there is a finite bound on the their generalization error! **Furthermore, this bound can be described in term of a quantity $(d_{vc})$, that solely depends on the hypothesis space and not on the distribution of the data points!\n",
    "\n",
    "Now, in light of these results, is there's any hope for the memorization hypothesis?\n",
    "\n",
    "It turns out that there’s still no hope! The memorization hypothesis can shatter any dataset no matter how big it is, that means that its $d_{vc}$ is infinite, yielding an infinite bound on $R(h_{mem})$ as before. However, the success of linear hypothesis can now be explained by the fact that they have a finite $d_{vc}=n+1$ in $\\mathbb{R}^n$. The theory is now consistent with the empirical observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Distribution-Based Bounds <a class=\"anchor\" id=\"3.3\"></a>\n",
    "\n",
    "The fact that $d_{vc}$ is distribution-free comes with a price: by not exploiting the structure and the distribution of the data samples, the bound tends to get loose. Consider for example the case of linear binary classifiers in a very higher $n$-dimensional feature space, using the distribution-free $d_{vc}=n+1$ means that the bound on the generalization error would be poor unless the size of the dataset $N$ is also very large to balance the effect of the large $d_{vc}$. This is the good old [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) we all know and endure.\n",
    "\n",
    "However, a careful investigation into the distribution of the data samples can bring more hope to the situation. For example, For data points that are linearly separable, contained in a ball of radius $R$, with a margin $\\rho$ between the closest points in the two classes, one can prove that for a hyperplane classifier:\n",
    "\n",
    "$$\n",
    "d_\\mathrm{vc} \\leq \\left\\lceil \\frac{R^2}{\\rho^2} \\right\\rceil\n",
    "$$\n",
    "\n",
    "It follows that the larger the margin, the lower the $d_{vc}$ of the hypothesis. This is theoretical motivation behind **Support Vector Machines (SVMs)** which attempts to classify data using the maximum margin hyperplane. This was also proved by Vapnik and Chervonenkis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:<a class=\"anchor\" id=\"r\"></a>\n",
    "\n",
    "\n",
    "* [1] [05 Sep 2016 Machine Learning Theory - Part 1: Introduction by Author Mostafa Samir](https://mostafa-samir.github.io/ml-theory-pt1/)\n",
    "* [2] [26 Sep 2016 Machine Learning Theory - Part 2: Generalization Bounds by Author Mostafa Samir](https://mostafa-samir.github.io/ml-theory-pt2/)\n",
    "* [3] [An Interactive Journey into Machine Learning.](https://mlweb.loria.fr/book/en/errorbounds.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
