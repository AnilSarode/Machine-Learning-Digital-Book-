{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content:\n",
    "\n",
    "* [1. Probability Distribution](#1)\n",
    "* [1.1. Bernoulli Distribution.](#1.1)\n",
    "* [1.2. Maximum Likelihood for Bernoulli Distribution.](#1.2)\n",
    "* [1.3. Binomial Distribution](#1.3)\n",
    "* [1.4.  Multinoulli distribution /Categorical distribution](#1.4)\n",
    "* [1.5. Multinomial distribution](#1.5)\n",
    "* [2.Naive Bayes Classification](#2)\n",
    "* [2.1. Overview](#2.1)\n",
    "* [2.2. Posterior Probabilities ](#2.2)\n",
    "* [2.3. Class-conditional Probabilities](#2.3)\n",
    "* [2.4  Prior Probabilities](#2.4)\n",
    "* [2.5. Evidence](#2.5)\n",
    "* [2.6. Definition of Naive Bayes(NB) Model ](#2.6)\n",
    "* [2.7. Maximum-Likelihood estimates for the Naive Bayes Model ](#2.7)\n",
    "* [3.Multinomial Naive Bayes - A Toy Example](#3)\n",
    "* [3.1. Maximum-Likelihood Estimates ](#3.1)\n",
    "* [3.2.Classification](#3.2)\n",
    "* [3.3. Additive Smoothing](#3.3)\n",
    "* [4. Naive Bayes and Text Classification](#4)\n",
    "* [4.1. The Bag of Words Model](#4.1)\n",
    "* [4.2 Multi-variate Bernoulli Naive Bayes](#4.2)\n",
    "* [4.3.Multinomial Naive Bayes](#4.3)\n",
    "* [5. Naive Bayes implementation for sentiment analysis](#5)\n",
    "* [5.1. Now we perform the text preprocessing ](#5.1)\n",
    "* [5.2. Vectorization](#5.2)\n",
    "* [5.3.Multinomial Naive Bayes Model-Sklearn](#5.3)\n",
    "* [5.4.Bernoulli Naive Bayes Model -Sklearn](#5.4)\n",
    "* [5.5. Performances of the Multi-variate Bernoulli and Multinomial Model](#5.5)\n",
    "* [References](#r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gratitude\n",
    "\n",
    "Before going ahead I would like thanks to many people who help me indirectly to write this Chapter/Blog. To write this I have read and used many references which are included in the [References.](#r) (The important one are from 11 to 18)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Probability Distribution  <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "Before dive into Naive Bayes Classification algorithm I would like to discuss some of the important Probability Distribution.In that *Bernoulli Distribution* and *multinomial Distribution* are important one. *Gaussian/Normal distribution* we already learn in the last chapter [Probability Theory 2.](https://github.com/AnilSarode/Machine-Learning-Digital-Book-/blob/master/Probability%20Theory%202/Probability%20Theory%202.ipynb) Here we will try get basic difference between the some of the standard discrete distribution.      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Bernoulli Distribution. <a class=\"anchor\" id=\"1.1\"></a>\n",
    "\n",
    "Suppose in the toss of a coin we are counting heads.if we get head we score one point and when we get tail we score no point.If X is random variable which denote the number of heads obtained (In this case one head only ) then the tossing of the coin provide the Bernoulli Distribution. $P(X=1)=p$ and $P(X=0)=q$,where $p+q=1$.\n",
    "\n",
    "Consider one more example Suppose in the toss die if we get even number, the score is one and if we get odd number the score is zero.If $X$ denotes the score then $X$ has Bernoulli's Distribution.$P(X=1)=p$ and $P(X=0)=q$ where $p+q=1$.\n",
    "\n",
    "Recall that a Bernoulli distribution, $P(X=x;p)$ is a simple binary distribution over random variables taking values in $x \\in \\{0,1\\}$, parameterized by $p$, which is just the probability of the random variable being equal to one.\n",
    "\\begin{equation}\n",
    "P(X = x; p) = p^x (1-p)^{(1-x)}\n",
    "\\end{equation}\n",
    "\n",
    "Now we will consider the **Multi-variate Bernoulli Distribution**\n",
    "\n",
    "Assume $\\{X_i, i= 1, 2, . . . . m\\}$ is a sequence of Bernoulli random variables, i.e., for $i = 1, 2, . . . . m,$\n",
    "\n",
    "$$\n",
    "P(X_i =1)=p \\hspace{0.5cm} P(X_i=0) =q ={1-p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(X_i = x_i; p)= \\prod_{i=1}^m p^{ x_i }(1-p)^{1-x_i} \\cdot\\cdot\\cdot (A)\n",
    "$$\n",
    "\n",
    "Here we assume that each Random variable $X_i$ for $i = 1, 2, . . . . m,$ is independent.So we can take the multiplication of all and $x_i \\in \\{0,1\\}$\n",
    "\n",
    "\n",
    "\n",
    "In summary,say you are flipping a coin that has a probability of 0.4 of turning up heads and 0.6 of turning up tails. The simplest probability distribution there is is the Bernoulli distribution which just asks for the probability of the coin turning up heads after one toss, that is, 0.4. That's all. The Bernoulli distribution is only about the probability of a random event with two possible outcomes occurring after one trial. It's used for success/fail type random variables where you want to reason about the probability that a random variable will succeed (such as a coin turning up heads).\n",
    "\n",
    "The probability mass function for the Bernoulli distribution is:\n",
    "\n",
    "\\begin{equation}\n",
    "P(X = x; p) = p^x (1-p)^{(1-x)}\n",
    "\\end{equation}\n",
    "\n",
    "where $p$ is the probability of a success **and $x$ is the number of successes desired**, which can be either 1 or 0. When $x$ is 1 then you find the probability of a success whilst when it is 0 then you find the probability of a fail. Since there are only two outcomes, if $p$ is the probability of a success then the probability of a non-success is $1-p$. Notice that this function is quite silly really as it is just a closed form expression for choosing either $p$ or $1-p$ depending on what $x$ is. The number you're interested in is raised to the power of 1 whilst the number you're not interested in is raised to the power of 0, turning it into 1, and then the two results are multiplied together.\n",
    "\n",
    "You can know more about [Bernoulli Distribution.](https://en.wikipedia.org/wiki/Bernoulli_distribution#Related_distributions)\n",
    "\n",
    "### 1.2. Maximum Likelihood for Bernoulli Distribution.<a class=\"anchor\" id=\"1.2\"></a>\n",
    "\n",
    "Before going ahead we had learn about MLE i.e.Maximum likelihood Estimation in chapter [Probability Theory 2.](https://github.com/AnilSarode/Machine-Learning-Digital-Book-/blob/master/Probability%20Theory%202/Probability%20Theory%202.ipynb) So here I will assume you had gone through the this chapter.\n",
    "\n",
    "Let's take a simple example as an illustration of this point for the Bernoulli distribution.  just now we saw that a Bernoulli distribution, $P(X=x;p)$ is a simple binary distribution over random variables taking values in $ x \\in \\{0,1\\}$, parameterized by $p$, which is just the probability of the random variable being equal to one. Now suppose we have some data (independent values) $x_{1}, \\ldots, x_{m}$ with $x_{i} \\in \\{0,1\\})$; what would be a good estimate of the Bernoulli parameter $p$?  For example, maybe we flipped a coin 100 times and 30 of these times it came up heads; what would be a good estimate for the probability that this coin comes up heads?\n",
    "\n",
    "The \"obvious\" answer here is that we just estimate $p$ to be the proportion of 1's in the data\n",
    "\\begin{equation}\n",
    "p = \\frac{\\mbox{# 1's}}{\\mbox{# Total}} = \\frac{\\sum_{i=1}^m x_{i}}{m}.\n",
    "\\end{equation}\n",
    "But why is this the case?  If we flip the coin just once, for example, would we expect that we should estimate $p$ to be either zero or one?  Maybe some other estimators exist that can better handle our expectation that the coin \"should\" be unbiased, i.e., have $p = 1/2$.\n",
    "\n",
    "**While this is certainly true, in fact that maximum likelihood estimate of $p$ _is_ just the equation above, the number of ones divided by the total number.**  So this gives some rationale that at least under the principles of maximum likelihood estimation, we should believe that this is a good estimate.  However, showing that this is in fact the maximum likelihood estimator is a little more involved that you might expect.  Let's go through the derivation to see how this work.\n",
    "\n",
    "First, recall that our objective is to choose $p$ maximize the likelihood, or equivalently the log likelihood of the data, of the observed data $x_{1}, \\ldots, x_{m}$.  This can be written as the optimization problem\n",
    "\n",
    "or simply take the log on both the side of the equation (A)\n",
    " \n",
    " \n",
    "\\begin{equation}\n",
    "\\max_{p} \\sum_{i=1}^m \\log P(X=x_{i};p).\n",
    "\\end{equation}\n",
    "\n",
    "Recall that the probability under a Bernoulli distribution for single random variable is just $P(X=1;p) = p$, and $P(X=0;p)=1 - p$, which we can write compactly as\n",
    "\n",
    "\\begin{equation}\n",
    "P(X = x; p) = p^x (1-p)^{(1-x)}\n",
    "\\end{equation}\n",
    "\n",
    "(it's easy to see that this equals $p$ for $x=1$ and $1-p$ for $x=0$).  Plugging this in to our maximum likelihood optimization problem we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{p} \\sum_{i=1}^m \\left (x_{i}\\log p + (1-x_{i}) \\log (1-p) \\right )\n",
    "\\end{equation}\n",
    "\n",
    "In order to maximize this equation, let's take the derivative and set it equal to 0 (though we won't show it, it turns out this function just a single maximum point, which thus must have derivative zero, and so we can find it in this manner).  Via some basic calculus we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{d}{d p} \\sum_{i=1}^m \\left (x_{i}\\log p + (1-x_{i}) \\log (1-p) \\right ) \n",
    "& = \\sum_{i=1}^m \\frac{d}{d p} \\left ( x_{i}\\log p + (1-x_{i}) \\log (1-p) \\right )  \\\\\n",
    "& = \\sum_{i=1}^m \\left ( \\frac{x_{i}}{p} - \\frac{1-x_{i}}{1-p} \\right )\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Setting this term equal to zero we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "& \\sum_{i=1}^m \\left ( \\frac{x_{i}}{p} - \\frac{1-x_{i}}{1-p} \\right ) = 0 \\\\\n",
    "\\Longrightarrow \\;\\; & \\frac{\\sum_{i=1}^m x_{i}}{p} - \\frac{\\sum_{i=1}^m (1-x_{i})} {1-p} = 0 \\\\\n",
    "\\Longrightarrow \\;\\; & (1-p) \\sum_{i=1}^m x_{i}  - p \\sum_{i=1}^m (1-x_{i}) = 0 \\\\\n",
    "\\Longrightarrow \\;\\; & \\sum_{i=1}^m x_{i} = p \\sum_{i=1}^m (x_{i} + (1-x_{i})) \\\\\n",
    "\\Longrightarrow \\;\\; & \\sum_{i=1}^m x_{i} = p m \\\\\n",
    "\\Longrightarrow \\;\\; & p = \\frac{\\sum_{i=1}^m x_{i}}{m}.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "**And there we have it, the surprisingly long proof of the fact that if we want to pick $p$ to maximize the likelihood of the observed data, we need to choose it to be equal to the empirical proportion of the ones**. \n",
    "\n",
    "**Note:** keep the above point in mind \n",
    "\n",
    "Of course, the objections we had at the beginning of this section were also valid: and in fact this perhaps is _not_ the best estimate of $p$ if we have very little data, or some prior information about what values $p$ should take.  But it _is_ the estimate of $p$ that maximizes the probability of the observed data, and if this is a bad estimate then it reflects more on the underlying problem with this procedure than with the proof above.  Nonetheless, in the presence of a lot of data, there is actually good reason to use the maximum likelihood estimator, and it is extremely common to use in practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Binomial Distribution <a class=\"anchor\" id=\"1.3\"></a>\n",
    "\n",
    "One generalization we can make upon the Bernoulli distribution is to consider multiple trails instead of just one and then ask about the probability that a number of successes would occur. This is called the binomial distribution. For example, what is the probability that the coin described above results in exactly 2 heads after tossing it 3 times. There are many ways how this result can come about. You can get [head, head, tail], or [head, tail, head], or [tail, head, head]. For this reason we need to consider the number of different ways the required total can be achieved, which is found by using the combination formula.\n",
    "\n",
    "The probability mass function for the Binomial distribution is:\n",
    "\n",
    "$$\n",
    "P(x;n,p)= \\frac{n!}{x!{(n-x)!}} p^{x} (1-p)^{(n-x)} =  \\binom{n}{x}  p^{x} (1-p)^{(n-x)}\n",
    "$$\n",
    "\n",
    "where  $p$ is the probability of a success, $n$ is the number of trials to try out, and $x$ is the number of desired successes out of the $n$ trials.\n",
    "\n",
    "You can know more about [Binomial distribution.](https://en.wikipedia.org/wiki/Binomial_distribution) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.  Multinoulli distribution /Categorical distribution <a class=\"anchor\" id=\"1.4\"></a>\n",
    "\n",
    "Whereas the binomial distribution generalizes the Bernoulli distribution across the number of trials, the multinoulli distribution generalizes it across the number of outcomes, that is, rolling a dice instead of tossing a coin. The multinoulli distribution is also called the categorical distribution.\n",
    "\n",
    "The probability mass function for the multinoulli distribution is:\n",
    "\n",
    "$$\n",
    "P(x_{s};p_{s}) = p_{s_{1}}^{x_{s_{1}}} \\times  p_{s_{2}}^{x_{s_{2}}}\\times \\cdot \\cdot \\cdot \\times p_{s_{k}}^{x_{s_{k}}}  \n",
    "$$\n",
    "\n",
    "Where $k$ is the number of outcomes (6 in the case of a dice, 2 for a coin), $p_{s}$ is the list of $k$ probabilities where $p_{s_{i}}$ is the probability of the $i^{th}$ outcome resulting in a success, $x_{s}$ is a list of numbers of successes, where $x_{s_{i}}$ is the number of successes desired for the $i^{th}$ outcome, which can be either 1 or 0 and where there can only be exactly a single \"1\" in $x_{s}$.\n",
    "\n",
    "The above equation can be written in more compact form as\n",
    "\n",
    "$$\n",
    "P(x_{1} \\cdot\\cdot\\cdot\\cdot x_{k} ,p_{1}\\cdot\\cdot\\cdot\\cdot p_{k})=\\prod_{i=1}^{k} p_{i}^{[x_{i}=i]}\n",
    "$$\n",
    "\n",
    "where $[x_{i}=i]$ evaluates to 1 if $x_{i}=i$, 0 otherwise.\n",
    "\n",
    "You can know more about [Categorical distribution.](https://en.wikipedia.org/wiki/Categorical_distribution) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Multinomial distribution<a class=\"anchor\" id=\"1.5\"></a>\n",
    "\n",
    "Finally the most general generalization of the Bernoulli distribution is across both the number of trials and the number of outcomes, called the multinomial distribution. In order to be more general, this distribution also allows specifying the number of successes desired for each individual outcome, rather than just of one outcome like the multinoulli distribution. This lets us calculate the probability of rolling a dice 6 times and getting Three $2s$, Two$3s$ and a $5$. Since we want it to be compatible with the binomial distribution, these outcomes can arrival in any order, that is, it doesn't matter whether you get [5,3,2,3,2,2] or [2,2,2,3,3,5]. For this reason we need to use the combination function again.\n",
    "$$\n",
    "\\begin{align}\n",
    "P(x_{1} \\cdot\\cdot\\cdot\\cdot x_{m} ,p_{1}\\cdot\\cdot\\cdot\\cdot p_{m} )= {{n}\\choose{x_1, ..., x_m}}\\prod_{i=1}^m p_i^{x_i} \\\n",
    "= n! \\prod_{i=1}^m \\frac{p_i^{x_i}}{x_i!}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and also\n",
    "$$\n",
    "\\sum_{i=1}^{m}x_{i}=n\n",
    "$$\n",
    "\n",
    "The likelihood function is \n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\mathbf{p}) &= {{n}\\choose{x_1, ..., x_m}}\\prod_{i=1}^m p_i^{x_i} \\\\\n",
    "&= n! \\prod_{i=1}^m \\frac{p_i^{x_i}}{x_i!}\n",
    "\\end{align}\n",
    "$$\n",
    "and the log-likelihood is\n",
    "\n",
    "\\begin{align}\n",
    "l(\\mathbf{p}) = \\log L(\\mathbf{p}) \n",
    "&= \\log \\bigg( n! \\prod_{i=1}^m \\frac{p_i^{x_i}}{x_i!} \\bigg)\\\\\n",
    "&= \\log n! + \\log \\prod_{i=1}^m \\frac{p_i^{x_i}}{x_i!} \\\\\n",
    "&= \\log n! + \\sum_{i=1}^m \\log \\frac{p_i^{x_i}}{x_i!} \\\\\n",
    "&= \\log n! + \\sum_{i=1}^m x_i \\log p_i - \\sum_{i=1}^m \\log x_i!\n",
    "\\end{align}\n",
    "\n",
    "Posing a constraint$\\sum_{i=1}^m p_i = 1$ with Lagrange multiplier\n",
    "$$\n",
    "\\begin{align}\n",
    "l'(\\mathbf{p},\\lambda) &= l(\\mathbf{p}) + \\lambda\\bigg(1 - \\sum_{i=1}^m p_i\\bigg)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To find \n",
    "$$\n",
    "\\arg\\max_\\mathbf{p} L(\\mathbf{p},\\lambda)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial p_i} l'(\\mathbf{p},\\lambda) \n",
    "= \\frac{\\partial}{\\partial p_i} l(\\mathbf{p})\n",
    "+ \\frac{\\partial}{\\partial p_i} \\lambda\\bigg(1 - \\sum_{i=1}^m p_i\\bigg) &= 0\\\\\n",
    " \\frac{\\partial}{\\partial p_i} \\sum_{i=1}^m x_i \\log p_i\n",
    "- \\lambda \\frac{\\partial}{\\partial p_i} \\sum_{i=1}^m p_i &= 0 \\\\\n",
    "\\frac{x_i}{p_i}- \\lambda  &= 0 \\\\\n",
    "p_i &= \\frac{x_i}{\\lambda} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus \n",
    "$$\n",
    "\\begin{align}\n",
    "p_i &= \\frac{x_i}{n}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "because \n",
    "$$\n",
    "\\begin{align}\n",
    "p_i &= \\frac{x_i}{\\lambda} \\\\\n",
    "\\sum_{i=1}^m p_i &= \\sum_{i=1}^m \\frac{x_i}{\\lambda} \\\\\n",
    "1 &= \\frac{1}{\\lambda} \\sum_{i=1}^m x_i \\\\\n",
    "\\lambda &= n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finally, the most likely probability distribution is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{p} = \\bigg(\n",
    "\\frac{x_1}{n},\n",
    "...,\n",
    "\\frac{x_m}{n}\n",
    "\\bigg)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Again the it is a normal probability ratio.Thanks to [this](https://math.stackexchange.com/questions/421105/maximum-likelihood-estimator-of-parameters-of-multinomial-distribution) \n",
    "\n",
    "You can know more about [Multinomial distribution.](https://en.wikipedia.org/wiki/Multinomial_distribution) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes Classification <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "### 2.1 Overview <a class=\"anchor\" id=\"2.1\"></a>\n",
    "\n",
    "Naive Bayes classifiers are linear classifiers that are known for being simple yet very efficient. The probabilistic model of naive Bayes classifiers is based on Bayes' theorem, and the adjective naive comes from the assumption that the features in a dataset are mutually independent. In practice, the independence assumption is often violated, but naive Bayes classifiers still tend to perform very well under this unrealistic assumption [1](#r1). Especially for small sample sizes, naive Bayes classifiers can outperform the more powerful alternatives [2](#r2).\n",
    "\n",
    "Being relatively robust, easy to implement, fast, and accurate, naive Bayes classifiers are used in many different fields. Some examples include the diagnosis of diseases and making decisions about treatment processes [3](#r3), the classification of RNA sequences in taxonomic studies [4](#r4), and Spam filtering in e-mail clients [5](#r5). However, strong violations of the independence assumptions and non-linear classification problems can lead to very poor performances of naive Bayes classifiers. We have to keep in mind that the type of data and the type problem to be solved dictate which classification model we want to choose. In practice, it is always recommended to compare different classification models on the particular dataset and consider the prediction performances as well as computational efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Posterior Probabilities <a class=\"anchor\" id=\"2.2\"></a>\n",
    "\n",
    "\n",
    "In order to understand how naive Bayes classifiers work, we have to briefly recapitulate the concept of Bayes' rule which we already saw in [Probability Theory 1](https://github.com/AnilSarode/Machine-Learning-Digital-Book-/blob/master/Probability%20Theory%201/Probability%20Theory%201.ipynb). The probability model that was formulated by Thomas Bayes (1701-1761) is quite simple yet powerful; it can be written down in simple words as follows:\n",
    "$$\n",
    " posterior \\hspace{0.1cm} probability = \\frac{(Conditional Probability)(prior probability) }{evidence} \n",
    "$$\n",
    "\n",
    "Bayes' theorem forms the core of the whole concept of naive Bayes classification. The posterior probability, in the context of a classification problem, can be interpreted as: \"What is the probability that a particular object belongs to class $i$ given its observed feature values?\" A more concrete example would be: \"What is the probability that a person has diabetes given a certain value for a pre-breakfast blood glucose measurement and a certain value for a }post-breakfast blood glucose measurement?\"\n",
    "\n",
    "$$\n",
    "P(diabetes|xi) ;\\hspace{0.2cm} \\mathbf x_i = [ 90mg/dl; 145mg/dl]\n",
    "$$\n",
    "\n",
    "Let \n",
    "\n",
    "* $\\mathbf x_{j}^{i}$ be the feature vector of $d$ dimension of sample or training data $i$,$i\\in \\{1,2,\\ldots,n\\}$ and $j \\in \\{1,\\ldots,d\\}$  \n",
    "* $w_{k}^{i}$ be the notation of class  $k$, $k\\in \\{1,2,\\ldots,m\\}$ and $i$ is length of the training data and $m$ is number of label/class.\n",
    "* and $P(\\mathbf x_{j} | w_k)$ be the probability of observing sample $\\mathbf x_j$ given that is belongs to class $w_k$.\n",
    "\n",
    "The general notation of the posterior probability can be written as\n",
    "\n",
    "$$\n",
    "P(w_k\\mid \\mathbf x_j)=\\frac{P(x_j|w_k)\\cdot P(w_k)}{P(x_j)}   \n",
    "$$\n",
    "\n",
    "The objective function in the naive Bayes probability is to maximize the posterior probability or joint probability distribution  given the training data in order to formulate the decision rule.\n",
    "\n",
    "$$\n",
    "\\mathit {predicted class label} \\leftarrow \\underset{{k=1,\\ldots,m}}{\\operatorname{argmax}} {P(w_k\\mid \\mathbf x_j)} \\ldots \\ldots (A)\n",
    "$$\n",
    "\n",
    "We can also write joint probability \n",
    "\n",
    "$$\n",
    "\\mathit {predicted class label} \\leftarrow \\underset{{k=1,\\ldots,m}}{\\operatorname{argmax}} {P(w_k, x_{1},\\ldots,x_{d})} \\ldots\\ldots\\ldots(B)\n",
    "$$\n",
    "\n",
    "Where $d$ is number of features.It will get clear as we explore more.  \n",
    "\n",
    "To continue with our example above, we can formulate the decision rule based on the posterior probabilities as follows:\n",
    "\n",
    "person has diabetes if\n",
    "$$\n",
    "P(diabetes \\mid x_i) ≥ P(not-diabetes \\mid x_i);\n",
    "$$\n",
    "else classify person as healthy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Class-conditional Probabilities <a class=\"anchor\" id=\"2.3\"></a>\n",
    "\n",
    "One assumption that Bayes classifiers make is that the samples are i.i.d. The abbreviation i.i.d. stands for \"independent and identically distributed\" and describes random variables that are independent from one another and are drawn from a similar probability distribution. Independence means that the probability of one observation does not affect the probability of another observation (e.g., time series and network graphs are not independent). One popular example of i.i.d. variables is the classic coin tossing: The first coin flip does not affect the outcome of a second coin flip and so forth. Given a fair coin, the probability of the coin landing on \"heads\" is always 0.5 no matter of how often the coin if flipped. \n",
    "\n",
    "An additional assumption of naive Bayes classifiers is the conditional independence of features. Under this naive assumption, the class-conditional probabilities or (likelihoods) of the samples can be directly estimated from the training data instead of evaluating all possibilities of $\\mathbf x$. Thus, given a $d$-dimensional feature vector $\\mathbf x$, the class conditional probability can be calculated as follows:\n",
    "\n",
    "$$\n",
    "P(\\mathbf x_j|w_k)=   P( x_{1}|w_k) P( x_{2}|w_k)\\ldots\\ldots P( x_{d}|w_k)=\\prod_{j=1}^{d}P( x_{j}|w_k) \\ldots \\dots \\ldots (C) \n",
    "$$\n",
    "\n",
    "Here, $P(\\mathbf x_j \\mid w_k)$ simply means: \"How likely is it to observe this particular pattern $\\mathbf x$ given that it belongs to class $w_k$?\" The \"individual\" likelihoods for every feature in the feature vector can be estimated via the **maximum-likelihood estimate**, which is simply a frequency in the case of categorical data:(I will prove this later section 2.7 as we proved for Bernoulli Distribution and Multinomial Distribution)\n",
    "\n",
    "$$\n",
    "\\hat P(x_j\\mid w_k)=\\frac{N_{x_{j},{w_{k}}}}{N_{w_{k}}} \\hspace{0.5cm}{(j=1,\\ldots,d)} \\ \\ldots \\dots \\ldots (D)\n",
    "$$\n",
    " \n",
    "* $N_{x_{j},{w_{k}}}:$ Number of times feature $x_j$ appears in samples from class $w_k$.\n",
    "* $N_{w_{k}}:$ Total count of all features in class $w_k$.\n",
    "\n",
    "\n",
    "To illustrate this concept with an example, let’s assume that we have a collection of 500 documents where 100 documents are *spam messages*. Now, we want to calculate the class-conditional probability for a new message \"Hello World\" given that it is *spam*. Here, the pattern consists of two features: *hello* and *world* and the class-conditional probability is the product of the \"probability of encountering *hello* given the message is *spam*\" and the probability of encountering *world* given the message is *spam*\".\n",
    "\n",
    "$$\n",
    "P(x = [hello, world] \\mid w = spam) = P(hello \\mid spam) \\cdot P(world \\mid spam)\n",
    "$$\n",
    "\n",
    "Using the training dataset of 500 documents, we can use the Maximum-likelihood estimate to estimate those probabilities: We'd simply calculate how often the words occur in the corpus of all spam messages.Suppose *hello* word occur 20 times in the samples of class *spam* and *world* word occur only 2 times in the samples of class *spam*  \n",
    "\n",
    "$$\n",
    "P(x = [hello, world] \\mid w = spam)=\\frac{20}{100}\\cdot \\frac{2}{100}=0.004\n",
    "$$\n",
    "\n",
    "However, with respect to the naive assumption of conditional independence, we notice a problem here: The naive assumption is that a particular word does not influence the chance of encountering other words in the same document. For example, given the two words \"peanut\" and \"butter\" in a text document, intuition tells us that this assumption is obviously violated: If a document contains the word \"peanut\" it will be more likely that it also contains the word \"butter\" (or \"allergy\"). In practice, the conditional independence assumption is indeed often violated, but naive Bayes classifiers are known to perform still\n",
    "well in those cases [6](#r6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Prior Probabilities <a class=\"anchor\" id=\"2.4\"></a>\n",
    "\n",
    "In contrast to a frequentist's approach, an additional prior probability (or just prior) is introduced that can be interpreted as the prior belief or a priori knowledge as we had saw in the chapter [Probability Theory 1](https://github.com/AnilSarode/Machine-Learning-Digital-Book-/blob/master/Probability%20Theory%201/Probability%20Theory%201.ipynb). \n",
    "\n",
    "$$\n",
    " posterior \\hspace{0.1cm} probability = \\frac{(Conditional Probability)(prior probability) }{evidence} \n",
    "$$\n",
    "\n",
    "In the context of pattern classification, the prior probabilities are also called class priors, which describe \"the general probability of encountering a particular class.\" In the case of spam classification, the priors could be formulated as\n",
    "\n",
    "$$\n",
    "P(spam) = \"the \\hspace{0.2cm} probability \\hspace{0.2cm} that\\hspace{0.2cm} any\\hspace{0.2cm} new\\hspace{0.2cm} message\\hspace{0.2cm} is\\hspace{0.2cm} a\\hspace{0.2cm} spam\\hspace{0.2cm} message\"\n",
    "$$\n",
    "and\n",
    "$$\n",
    "P(Not spam) = 1 − P(spam)\n",
    "$$\n",
    "\n",
    "**If the priors are following a uniform distribution, the posterior probabilities will be entirely determined by the class-conditional probabilities and the evidence term. And since the evidence term is a constant, the decision rule willentirely depend on the class-conditional probabilities (similar to a frequentist's approach and maximum-likelihood estimate).**\n",
    "\n",
    "Eventually, the a priori knowledge can be obtained, e.g., by consulting a domain expert or by estimation from the training data (assuming that the training data is i.i.d. and a representative sample of the entire population. The\n",
    "maximum-likelihood estimate approach can be formulated as\n",
    "\n",
    "$$\n",
    "\\hat P(w_k)=\\frac{N_{w_{k}}}{N_c} \\ldots \\dots \\ldots (E)\n",
    "$$\n",
    "\n",
    "* $N_{w_{k}}:$ Total count of all features/samples in class $w_k$.\n",
    "* $N_{c}:$ Total count of all the features/samples.\n",
    "\n",
    "And in context of spam classification:\n",
    "$$\n",
    "    \\hat P(spam)=\\frac{of \\ spam \\ messages\\ in \\ training \\ data}{of \\ all \\ messages \\ in \\ training \\ data}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Evidence <a class=\"anchor\" id=\"2.5\"></a>\n",
    "\n",
    "After defining the class-conditional probability and prior probability, there is only one term missing in order to compute posterior probability, that is the evidence.\n",
    "\n",
    "$$\n",
    " posterior \\hspace{0.1cm} probability = \\frac{(Conditional Probability)(prior probability) }{evidence} \n",
    "$$\n",
    "\n",
    "The evidence $P(\\mathbf x)$ can be understood as the probability of encountering a particular pattern $\\mathbf x$ independent from the class label.Given the more formal definition of posterior probability\n",
    "\n",
    "\n",
    "$$\n",
    "P(w_k\\mid \\mathbf x_j)=\\frac{P(x_j|w_k)\\cdot P(w_k)}{P(x_j)}   \n",
    "$$\n",
    "\n",
    "\n",
    "the evidence can be calculated as follows ($w_{k}^{C}$ stands for \"complement\"and basically translate to \"**Not** class $w_{k}$\".): \n",
    "\n",
    "$$\n",
    "P(x_{j})=P(x_j\\mid w_k)\\cdot P(w_k)+P(x_j\\mid w_{k}^{C})\\cdot P(w_{k}^{C})\n",
    "$$\n",
    "\n",
    "This is nothing but the **marginal probability** which we had learn in chapter [Probability theory](https://github.com/AnilSarode/Machine-Learning-Digital-Book-/blob/master/Probability%20Theory%201/Probability%20Theory%201.ipynb) Although the evidence term is required to accurately calculate the posterior probabilities, it can be removed from the decision rule \"Classify sample $\\mathbf x_j$ as $w_1$ if $ P(w_1 \\mid x_j) > P(w_2 \\mid x_j)$ else classify the sample as $w_2$,\" since it is merely a scaling factor:\n",
    "\n",
    "$$\n",
    "\\frac{P(x_j|w_1)\\cdot P(w_1)}{P(x_j)} >\\frac{P(x_j|w_2)\\cdot P(w_2)}{P(x_j)} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\propto\tP(x_j|w_1)\\cdot P(w_1) >P(x_j|w_2)\\cdot P(w_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Definition of Naive Bayes(NB) Model  <a class=\"anchor\" id=\"2.6\"></a>\n",
    "\n",
    "As from section 2.5 we come to that evidence term i.e probability of given particular pattern $\\mathbf x $ dose not contribute in our decision rule also using equation $(A)$ ,$(B)$  and $(C)$  we can make the definition of Naive Bayes as \n",
    "\n",
    "*A NB model consists of an integer $k$ specifying the number of possible labels/class, an integer $d$ specifying the number of attributes/features, and in addition the following parameters*\n",
    "\n",
    "* A parameter \n",
    "\n",
    "$$\n",
    "P(w_{k})\n",
    "$$\n",
    "\n",
    "*for any $k \\in \\{1,2,\\ldots, m\\} $. The parameter $P(w_{k})$ can be interpreted as the probability of seeing the label/class $w_{k}$. We have the constraints  $P(w_{k}) \\geq 0$ and $\\sum_{k=1}^{m}P(w_{k})=1$ and $m\\geq 2$ and for $m=2$ the $k$ will take only two(binary) values for class $w_1$ and $w_2$ respectively*\n",
    "\n",
    "\n",
    "* A parameter \n",
    "$$\n",
    "P(x_j|w_k)\n",
    "$$\n",
    "\n",
    "for any $j \\in \\{1,2,\\ldots,d\\}$ and $k \\in \\{1,2,\\ldots, m\\} $.\n",
    "\n",
    "\n",
    "We then define the probability for any $w_k$ for $x_1\\ldots x_d$ as \n",
    "\n",
    "$$\n",
    "{P(w_k, x_{1},\\ldots,x_{d})}=P(w_k)\\prod_{j=1}^{d}P(x_j|w_k)\n",
    "$$ \n",
    "\n",
    "Once these parameters have been estimated by MLE(Maximum Likelihood Estimation) , given a new test example \n",
    "$\\mathbf {\\bar x_j}=\\langle x_1,x_2,\\ldots,x_d \\rangle $, the output of the NB classifier is\n",
    "\n",
    "$$\n",
    "\\underset{{k=1,\\ldots,m}}{\\operatorname{argmax}} {P(w_k, x_{1},\\ldots,x_{d})}=\\underset{{k=1,\\ldots,m}}{\\operatorname{argmax}} {P(w_k)\\prod_{j=1}^{d}P(x_j|w_k)} \\ldots\\ldots (\\hat B) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Maximum-Likelihood estimates for the Naive Bayes Model <a class=\"anchor\" id=\"2.7\"></a>\n",
    "\n",
    "We now consider how the parameters $P(w_k)$ and $P(x_j|w_k)$ can be estimated from data. In particular, we will describe the maximum-likelihood estimates. We first state the form of the estimates, and then go into some detail about how the estimates are derived.\n",
    "\n",
    "Our training sample consists of examples $(x^{(i)}; w^{(i)})$ for $i = 1\\ldots n$. Recall that each $x^{(i)}$ is a $d$-dimensional vector.We write $x_{j}^{(i)}$ for the value of the $j^{th}$ component of $\\bar x^{(i)}; x_{j}^{(i)}$ can take values −1 or +1 then we are consider **Bernoulli Model** which means absence and presence of feature else we consider the occurrence count of the feature in **Multinomial Model**. This will get clear in the section [4.Naive Bayes and Text Classification.](#4)\n",
    "\n",
    "We already know that we reach equation $(D)$ and $(E)$ by using MLE,or In the other words we know that MLE estimation of the above two parameter is as by given by equation $(D)$ and $(E)$. \n",
    "\n",
    "$$\n",
    "\\hat P(x_j\\mid w_k)=\\frac{N_{x_{j},{w_{k}}}}{N_{w_{k}}} \\hspace{0.5cm}{(j=1,\\ldots,d)} \\ \\ldots \\dots \\ldots (D)\n",
    "$$\n",
    " \n",
    "* $N_{x_{j},{w_{k}}}:$ Number of times feature $x_j$ appears in samples from class $w_k$.\n",
    "* $N_{w_{k}}:$ Total count of all features in class $w_k$.\n",
    "\n",
    "$$\n",
    "\\hat P(w_k)=\\frac{N_{w_{k}}}{N_c} \\ldots \\dots \\ldots (E)\n",
    "$$\n",
    "\n",
    "* $N_{w_{k}}:$ Total count of all features/samples in class $w_k$.\n",
    "* $N_{c}:$ Total count of all the features/samples.\n",
    "\n",
    "\n",
    "The proof of the MLE estimation you can understand here [The Naive Bayes Model, Maximum-Likelihood\n",
    "Estimation, and the EM Algorithm](http://www.cs.columbia.edu/~mcollins/em.pdf).But as we saw in the initially for the for Bernoulli Distribution and for Multinomial Distribution the MLE estimation of the parameters i.e Probability for these two distribution is simply an empirical ratio calculation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Multinomial Naive Bayes - A Toy Example<a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "After covering the basics concepts of a naive Bayes classifier, the posterior probabilities and decision rules, let us walk through a simple toy example based on the training set shown in Figure 1.\n",
    "\n",
    "![](Toy_Multiple.png)\n",
    "$$\\mathbf {Figure \\ 1}$$\n",
    "\n",
    "A simple toy dataset of $12$ samples $2$ different classes $+$; $−$ . Each sample consists of $2$ features: color and geometrical shape.\n",
    "\n",
    "Let \n",
    "\n",
    "* $w_k$ be the class labels: $ w_k \\in {(+,-)}$\n",
    "* and $x_i$ be the $2$-dimensional feature vectors: $x_i = [x_{i1} x_{i2}]$; $x_{i1} \\in  \\{blue; green; red; yellow\\}$; $x_{i2} \\in \\{circle; square\\}$\n",
    "\n",
    "\n",
    "| $i$ | Color($j=1$)  | Shape($j=2$)  | Class($m=2$) |\n",
    "|-----| --------------| --------------| ------------ |\n",
    "|  1  |  Red          | Square        |   +          |\n",
    "|  2  |  Blue         | Square        |   -          |\n",
    "|  3  |  Green        | Square        |   +          |\n",
    "|  4  |  Blue         | Circle        |   +          |\n",
    "|  5  |  Blue         | Circle        |   +          |\n",
    "|  6  |  Blue         | Circle        |   -          |\n",
    "|  7  |  Green        | Square        |   +          |\n",
    "|  8  |  Red          | Square        |   +          |\n",
    "|  9  |  Blue         | Square        |   +          |\n",
    "| 10  |  Blue         | Square        |   -          |\n",
    "| 11  |  Green        | Circle        |   -          |\n",
    "| 12  |  Red          | Square        |   -          |\n",
    "\n",
    "\n",
    "\n",
    "The task now is to classify a new sample  pretending that we don't know that its true class label is \"$+$\":\n",
    "\n",
    "![](Toy1.png)\n",
    "$$\\mathbf {Figure \\ 2}$$\n",
    "\n",
    "\n",
    "A new sample from class $+$ and the features $x = [blue, square]$ as shown above figure 2, that is to be classified using the training data in above table.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Maximum-Likelihood Estimates <a class=\"anchor\" id=\"3.1\"></a>\n",
    "\n",
    "The decision rule can be defined as\n",
    "\n",
    "Classify sample as $+$ if\n",
    "\n",
    "$P(w = + \\mid x = [blue, square]) \\geq P(w = - \\mid x = [blue, square])$\n",
    "\n",
    "else classify sample as $-$.\n",
    "\n",
    "Under the assumption that the samples are i.i.d, the ***prior probabilities*** can be obtained via the maximum-likelihood estimate (i.e., *the frequencies of how often each class label is represented in the training dataset*):\n",
    "\n",
    "$$\n",
    "P(+)= \\frac{7}{12}=0.58\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(-)= \\frac{5}{12}=0.42\n",
    "$$\n",
    "\n",
    "Under the naive assumption that the features \"*color*\" and \"*shape*\" are mutually independent, the class-conditional probabilities can be calculated as a simple product of the individual conditional probabilities.\n",
    "\n",
    "Via maximum-likelihood estimate, e.g., $P(blue \\mid  -)$ is simply the frequency of observing a \"*blue*\" sample among all samples in the training dataset that belong to class $-$\n",
    "\n",
    "$$\n",
    "P(\\mathbf x\\mid +)=P(blue\\mid +)\\cdot P(sqaure\\mid +)= \\frac{3}{7}\\cdot\\frac{5}{7}=0.31\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "P(\\mathbf x\\mid -)=P(blue\\mid -)\\cdot P(sqaure\\mid -)= \\frac{3}{5}\\cdot\\frac{3}{5}=0.36\n",
    "$$\n",
    "\n",
    "Now, the posterior probabilities can be simply calculated as the product of the class-conditional and prior probabilities\n",
    "\n",
    "$$\n",
    "P(+\\mid \\mathbf x)=P(\\mathbf x\\mid +)\\cdot P(+)=0.31\\cdot 0.58=0.18\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "P(-\\mid \\mathbf x)=P(\\mathbf x\\mid -)\\cdot P(-)=0.36\\cdot 0.42=0.15\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.Classification<a class=\"anchor\" id=\"3.2\"></a>\n",
    "\n",
    "Putting it all together, the new sample can be classified by plugging in the posterior probabilities into the decision rule\n",
    "\n",
    "if $P(+\\mid \\mathbf x)\\geq P(-\\mid \\mathbf x)$\n",
    "\n",
    "classify as $+$,\n",
    "\n",
    "else classify as $-$\n",
    "\n",
    "\n",
    "Since $0.18 \\geq 0.15 $ the sample can be classified as $+$. Taking a closer look at the calculation of the posterior probabilities, this simple example demonstrates the effect of the prior probabilities affected on the decision rule. If the prior probabilities were equal for both classes, the new pattern would be classified as $-$\n",
    "instead of $+$. This observation also underlines the importance of representative training datasets; in practice, it is usually recommended to additionally consult a domain expert in order to define the prior probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Additive Smoothing<a class=\"anchor\" id=\"3.3\"></a>\n",
    "\n",
    "The classification was straight-forward given the sample in Figure 2. A trickier case is a sample that has a \"new\" value for the color attribute that is not present in the training dataset, e.g., yellow, as shown in Figure 3\n",
    "\n",
    "![](Toy2.png)\n",
    "$$\\mathbf {Figure \\ 3}$$\n",
    "\n",
    "A new sample from class $+$ and the features $x = [yellow, square]$ that is to be classified using the training data in Figure 1 \n",
    "\n",
    "If the color yellow does not appear in our training dataset, the class-conditional probability will be $P(\\mathbf x\\mid +)=0$ and $P(\\mathbf x\\mid -)=0$ as $P(yellow \\mid +)=0$, and $P(yellow\\mid -)=0$ and as a consequence, the posterior probability will also be $0$ since the posterior probability is the product of the prior and class-conditional probabilities.\n",
    "\n",
    "$$\n",
    "P(+\\mid \\mathbf x)=P(\\mathbf x\\mid +)\\cdot P(+)=0\\cdot 0.58=0.18\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "P(-\\mid \\mathbf x)=P(\\mathbf x\\mid -)\\cdot P(-)=0\\cdot 0.42=0.15\n",
    "$$\n",
    "\n",
    "\n",
    "In order to avoid the problem of zero probabilities, an additional smoothing term can be added to the multinomial Bayes model. The most common variants of additive smoothing are the so-called **Lidstone smoothing $(\\alpha < 1)$** and **Laplace smoothing $(\\alpha = 1)$**.\n",
    "\n",
    "$$\n",
    "\\hat P(x_j\\mid w_k)=\\frac{N_{x_{j},{w_{k}}}+\\alpha}{N_{w_{k}}+\\alpha d} \\hspace{0.5cm}{(j=1,\\ldots,d)}\n",
    "$$\n",
    "\n",
    "where,\n",
    "\n",
    "* $N_{x_{j},{w_{k}}}:$ Number of times feature $x_j$ appears in samples from class $w_k$.\n",
    "* $N_{w_{k}}:$ Total count of all features in class $w_k$.\n",
    "* $\\alpha$: Parameter for additive smoothing.\n",
    "* $d$: Dimensionality of the feature vector $\\mathbf x = [x_1, \\ldots , x_d]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes and Text Classification <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "This section will introduce some of the main concepts and procedures that are needed to apply the naive Bayes model to text classification tasks. Although the examples are mainly concerning a two-class problem classifying text messages as spam or ham  the same approaches are applicable to multi-class problems such as classification of documents into different topic areas (e.g., \"Computer Science\", \"Biology\", \"Statistics\", \"Economics\", \"Politics\", etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. The Bag of Words Model  <a class=\"anchor\" id=\"4.1\"></a>\n",
    "\n",
    "One of the most important sub-tasks in pattern classification are feature extraction and selection.Prior to fitting the model and using machine learning algorithms for training, we need to think about how to best represent a text document as a feature vector. A commonly used model in Natural Language Processing is the so-called\n",
    "**bag of words** model. The idea behind this model really is as simple as it sounds. \n",
    "\n",
    "We represent a text document bag-of-words as if it were a bag-of-words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. In the example in the Figure 4, instead of representing the word order in all the phrases like “I love this movie” and “I would recommend it”, we simply note that the word I occurred 5 times in the entire excerpt, the word it 6 times, the words love, recommend, and movie once, and so on\n",
    "\n",
    "<img src=\"bag.png\" alt=\"bag\" width=\"800\"/>\n",
    "\n",
    "\n",
    "$$\\mathbf {Figure \\ 4}$$\n",
    "\n",
    "In the $\\mathbf {Figure \\ 4}$ Intuition of the multinomial naive Bayes classifier applied to a movie review. The position of the words is ignored (the bag of words assumption) and we make use of the frequency of each word.\n",
    "\n",
    "First comes the creation of the vocabulary -the collection of all different words that occur in the training set.The vocabulary $V$ consists of the union of all the word types in all classes, not just the words in one class $w_k$.The vocabulary can then be used to construct the d-dimensional *feature vectors* for the individual documents where the dimensionality is equal to the number of different words in the vocabulary $(d =  |V|)$. This process is called *vectorization*.\n",
    "\n",
    "We shall look at two probabilistic models of documents, both of which represent documents as a bag of words, using the Naive Bayes assumption. Both models represent documents using feature vectors whose components correspond to word type.here type means what type of the distribution. \n",
    "\n",
    "**Bernoulli document model:** a document is represented by a feature vector with binary elements\n",
    "taking value 1 if the corresponding word is present in the document and 0 if the word is not\n",
    "present.\n",
    "\n",
    "**Multinomial document model:** a document is represented by a feature vector with integer elements\n",
    "whose value is the frequency of that word in the document.\n",
    "\n",
    "**Example:** Consider the vocabulary:\n",
    "\n",
    "$$\n",
    "V = \\{blue, red, dog, cat, biscuit, apple\\} .\n",
    "$$\n",
    "\n",
    "In this case $|V| = d = 6$. Now consider the (short) document \"the blue dog ate a blue biscuit\". If $\\mathbf x^{B}$ is the Bernoulli feature vector for this document, and If $\\mathbf x^{M}$ is the multinomial feature vector, then we would have \n",
    "\n",
    "$\\mathbf x^{B}=(1,0,1,0,1,0)^{T}$\n",
    "\n",
    "$\\mathbf x^{M}=(2,0,1,0,1,0)^{T}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Multi-variate Bernoulli Naive Bayes<a class=\"anchor\" id=\"4.2\"></a>\n",
    "\n",
    "As mentioned above, in the Bernoulli model a document is represented by a binary vector, which represents a point in the space of words. If we have a vocabulary $V$ containing a set of $|V|$ words, then the $j^{th}$ dimension/position of a feature vector corresponds to word  $W_j$ in the vocabulary. Let $\\mathbf x_{j}^{i}$ be the feature vector for the $i^{th}$ document $\\mathbf D^{i}$; then the $j^{th}$ element of $\\mathbf x_i$, written $x_{j}$, is either $0$ or $1$ representing the absence or presence of word $W_{j}$ in the $i^{th}$ document $\\mathbf D^{i}$.\n",
    "\n",
    "Then equation $(C)$ the class conditional probability will be \n",
    "$$\n",
    "P(\\mathbf x_j|w_k)=\\prod_{j=1}^{d=|V|} P(x_{j}\\mid w_k)^{x_{j}}\\cdot (1-P(x_{j}\\mid w_k))^{1-x_{j}}\\ldots \\ldots(\\hat C)  \\hspace{0.6cm} x_j \\in (0,1)\n",
    "$$\n",
    "\n",
    "This product goes over all words in the vocabulary. If word $x_{j}$ is present, then $x_{j}=1$ and the required\n",
    "probability is $P(x_{j}\\mid w_k)$; if word $x_{j}$ is not present, then $x_{j}= 0 $  and the required probability is $(1-P(x_{j}\\mid w_k)$.\n",
    "\n",
    "\n",
    "Let $\\hat P(x_j\\mid w_k)$ be the maximum-likelihood estimate that a particular word $x_j$ occurs in class $w_k$\n",
    "\n",
    "$$\n",
    "\\hat P(x_j\\mid w_k)=\\frac{N_{x_{j},{w_{k}}}+1}{N_{w_{k}}+\\alpha |V|} \\ldots (1)\n",
    "$$\n",
    "\n",
    " \n",
    "* $N_{x_{j},{w_{k}}}:$ is the number of documents in the training dataset that contain the\n",
    "feature $x_j$  and belong to class $w_k$.\n",
    "\n",
    "* $N_{w_{k}}:$ is the total number of documents in the training dataset that belong to class $w_k$.\n",
    "\n",
    "* $\\alpha$: Parameter for additive smoothing.\n",
    "* $|V|=d$: Dimensionality of the feature vector $\\mathbf x_{j} = [x_1, \\ldots , x_d]$.\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat P(w_k)=\\frac{N_{w_{k}}}{N_c} \\ldots (2) \n",
    "$$\n",
    "\n",
    "* $N_{w_{k}}:$ is the total number of documents in the training dataset that belong to class $w_k$.\n",
    "* $N_{c}:$ Total count of all the documents.\n",
    "\n",
    "\n",
    "To classify an unlabelled document $\\mathbf D^{i}$ which we can write as feature vector $\\mathbf x_{j}^{i}$ using vocabulary , then we estimate the posterior probability for each class combining\n",
    "equations $(\\hat C)$ and using equation $(A),(B)$ and $(\\hat B)$ \n",
    "\n",
    "$$\n",
    "{P(w_k\\mid \\mathbf x_j)}= \\prod_{j=1}^{d=|V|} P(x_{j}\\mid w_k)^{x_{j}}\\cdot (1-P(x_{j}\\mid w_k))^{1-x_{j}} \\cdot P(w_k) \\ldots \\ldots (3)  \\hspace{0.6cm} x_j \\in (0,1)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a set of documents, each of which is related either to Sports (S ) or to Informatics (I). Given\n",
    "a training set of $i=11$ documents $\\mathbf D^{i}$, we would like to estimate a Naive Bayes classifier, using the Bernoulli document model, to classify unlabelled documents as $w_1=S$ or $w_2=I$.using vocabulary \n",
    "\n",
    "e.g of $\\mathbf D^{1} $ is $\\mathbf D^{1}= after\\ goal\\ it\\ is\\ drink\\ break $ for class $w_1$\n",
    "\n",
    "$\\mathbf D^{2} =variance in performance $ for class $w_2$\n",
    "\n",
    "We define a vocabulary of eight words:\n",
    "\n",
    "$$\n",
    "V=\n",
    "\\begin{bmatrix}\n",
    "W_1 = goal\\\\\n",
    "W_2 = tutor\\\\\n",
    "W_3 = variance\\\\\n",
    "W_4 = speed\\\\\n",
    "W_5 = drink\\\\\n",
    "W_6 = defence\\\\\n",
    "W_7 = performance\\\\\n",
    "W_8 = field\\\\\n",
    "W_9 = break\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus each document $\\mathbf D^{i}$ is represented as an $9$-dimensional binary feature vector $\\mathbf x_{j}^{i}$ for $j=1,\\ldots,|V|$.\n",
    "\n",
    "The training data is presented below as a matrix for each class, in which each row represents document $D^{i}$ as a $9$-dimensional feature  vector\n",
    "\n",
    "$$\n",
    "\\mathbf X^{w_1=Sports}=\n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 1 & 1 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 1 & 0 & 1 & 1 & 0 & 1\\\\\n",
    "1 & 0 & 0 & 1 & 0 & 1 & 0 & 1 & 0\\\\\n",
    "1 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 1\\\\\n",
    "0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 1\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf X^{w_2=Info}=\n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 1 & 0 & 0 & 1 & 1 & 0\\\\\n",
    "0 & 1 & 1 & 0 & 0 & 1 & 0 & 0 & 1\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Classify the following into Sports or Informatics using a Naive Bayes classifier.\n",
    "\n",
    "1. $\\mathbf {\\bar x_{j}^{1}} = (1, 0, 0, 1, 1, 1, 0, 1, 1)^{T}$\n",
    "2. $\\mathbf {\\bar x_{j}^{2}} = (0, 1, 1, 0, 1, 0, 1, 0, 0)^{T}$\n",
    "\n",
    "Solution:\n",
    "\n",
    "The total number of documents in the training set ${N_c} =11$; $N_{w_{1}=S} =6$, $N_{w_{2}=I} =5$\n",
    "\n",
    "Using equation (2), we can estimate the prior probabilities from the training data as:\n",
    "\n",
    "$$\n",
    "P(w_1=S)=\\frac{6}{11} \\ ; \\hspace{1cm} P(w_2=I)=\\frac{5}{11} \n",
    "$$\n",
    "\n",
    "The word counts in the training data are:\n",
    "\n",
    "For class $w_1=S$\n",
    "\n",
    "$$\n",
    "N_{x_{1},{w_{1}}}=3 \\ ; \\hspace{1cm}  N_{x_{2},{w_{1}}}=1\n",
    "$$\n",
    "\n",
    "$$\n",
    "N_{x_{3},{w_{1}}}=2 \\ ; \\hspace{1cm}  N_{x_{4},{w_{1}}}=3 \n",
    "$$\n",
    "\n",
    "$$\n",
    "N_{x_{5},{w_{1}}}=3 \\ ; \\hspace{1cm}  N_{x_{6},{w_{1}}}=4\n",
    "$$\n",
    "\n",
    "$$\n",
    "N_{x_{7},{w_{1}}}=4 \\ ; \\hspace{1cm}  N_{x_{8},{w_{1}}}=4 \n",
    "$$\n",
    "\n",
    "$$\n",
    "N_{x_{9},{w_{1}}}=3  \n",
    "$$\n",
    "\n",
    "Similarly for class $w_2=I$\n",
    "\n",
    "$$\n",
    "N_{x_{1},{w_{2}}}=1 \\ ; \\hspace{1cm}  N_{x_{2},{w_{2}}}=3\n",
    "$$\n",
    "\n",
    "$$\n",
    "N_{x_{3},{w_{2}}}=3 \\ ; \\hspace{1cm}  N_{x_{4},{w_{2}}}=1\n",
    "$$\n",
    "\n",
    "$$\n",
    "N_{x_{5},{w_{2}}}=1 \\ ; \\hspace{1cm}  N_{x_{6},{w_{2}}}=1\n",
    "$$\n",
    "\n",
    "$$\n",
    "N_{x_{7},{w_{2}}}=3 \\ ; \\hspace{1cm}  N_{x_{8},{w_{2}}}=1\n",
    "$$\n",
    "\n",
    "$$\n",
    "N_{x_{9},{w_{2}}}=2\n",
    "$$\n",
    "\n",
    "The we can estimate the word likelihoods using equation (1)\n",
    "\n",
    "For class $w_1=S $\n",
    "\n",
    "$$\n",
    "\\hat P(x_1\\mid w_1)= \\frac{1}{2} \\ ; \\hspace{1cm} \\hat P(x_2\\mid w_1)= \\frac{1}{6}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat P(x_3\\mid w_1)= \\frac{1}{3} \\ ; \\hspace{1cm} \\hat P(x_4\\mid w_1)= \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat P(x_5\\mid w_1)= \\frac{1}{2} \\ ; \\hspace{1cm} \\hat P(x_6\\mid w_1)= \\frac{2}{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat P(x_7\\mid w_1)= \\frac{2}{3} \\ ; \\hspace{1cm} \\hat P(x_8\\mid w_1)= \\frac{2}{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat P(x_9\\mid w_1)= \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "Similarly for class $w_2=I$\n",
    "\n",
    "$$\n",
    "\\hat P(x_1\\mid w_2)= \\frac{1}{5} \\ ; \\hspace{1cm} \\hat P(x_2\\mid w_2)= \\frac{3}{5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat P(x_3\\mid w_2)= \\frac{3}{5} \\ ; \\hspace{1cm} \\hat P(x_4\\mid w_2)= \\frac{1}{5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat P(x_5\\mid w_2)= \\frac{1}{5} \\ ; \\hspace{1cm} \\hat P(x_6\\mid w_2)= \\frac{1}{5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat P(x_7\\mid w_2)= \\frac{3}{5} \\ ; \\hspace{1cm} \\hat P(x_8\\mid w_2)= \\frac{1}{5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat P(x_9\\mid w_2)= \\frac{2}{5}\n",
    "$$\n",
    "\n",
    "We use $(3)$ to compute the posterior probabilities of the two test vectors and hence classify them.\n",
    "\n",
    "1. $\\mathbf {\\bar x_{j}^{1}} = (1, 0, 0, 1, 1, 1, 0, 1, 1)^{T}$\n",
    "\n",
    "$$\n",
    "P(w_1\\mid \\mathbf {\\bar x_{j}})=\\prod_{j=1}^{d=9} P(x_{j}\\mid w_1)^{x_{j}}\\cdot (1-P(x_{j}\\mid w_1))^{1-x_{j}} P(w_1)  \\hspace{0.6cm} x_j \\in (0,1)\n",
    "$$\n",
    "**Note:** for simplicity I remove subscript $i$\n",
    "\n",
    "$$\n",
    "P(w_1\\mid \\mathbf {\\bar x_{j}})= \\Biggl(\\frac{1}{2}\\times\\frac{5}{6}\\times\\frac{2}{3}\\times\\frac{1}{2}\\times\\frac{1}{2}\\times\\frac{2}{3}\\times\\frac{1}{3}\\times\\frac{2}{3}\\times\\frac{1}{2}\\Biggr)\\times\\frac{6}{11}=2.8058\\times 10^{-3}\n",
    "$$\n",
    "\n",
    "Now for class $w_2$ \n",
    "\n",
    "$$\n",
    "P(w_2\\mid \\mathbf {\\bar x_{j}})=\\prod_{j=1}^{d=9} P(x_{j}\\mid w_2)^{x_{j}}\\cdot (1-P(x_{j}\\mid w_2))^{1-x_{j}} P(w_2)  \\hspace{0.6cm} x_j \\in (0,1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(w_2\\mid \\mathbf {\\bar x_{j}})= \\Biggl(\\frac{1}{5}\\times\\frac{2}{5}\\times\\frac{2}{5}\\times\\frac{1}{5}\\times\\frac{1}{5}\\times\\frac{1}{5}\\times\\frac{2}{5}\\times\\frac{1}{5}\\times\\frac{2}{5}\\Biggr)\\times\\frac{5}{11}=3.7236\\times 10^{-6}\n",
    "$$\n",
    "\n",
    "Classify this document as Sports.\n",
    "\n",
    "2. $\\mathbf {\\bar x_{j}^{2}} = (0, 1, 1, 0, 1, 0, 1, 0, 0)^{T}$\n",
    "\n",
    "Do it as self-study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.Multinomial Naive Bayes <a class=\"anchor\" id=\"4.3\"></a>\n",
    "\n",
    "We have already saw about the this in the [3.Multinomial Naive Bayes - A Toy Example](#3).Here we look for this in document classification point of view.Again the question comes how to estimate the two probabilities i.e prior $P(w_k)$ and $P(x_j \\mid w_k)$.? The prior $P(w_k)$  remains same as previously estimated for Bernoulli Distribution.For $P(x_j \\mid w_k)$  we simply use the frequencies in the data.It will get clear as we see the example.\n",
    "\n",
    "$$\n",
    "\\hat P(w_k)=\\frac{N_{w_{k}}}{N_c} \\ldots (2)\n",
    "$$\n",
    "\n",
    "* $N_{w_{k}}:$ is the total number of documents in the training dataset that belong to class $w_k$.\n",
    "* $N_{c}:$ Total count of all the documents.\n",
    "\n",
    "$$\n",
    "\\hat P(x_j\\mid w_k)=\\frac{Count(x_j,w_k)+1}{\\sum_{j=1}^{d=|v|} (Count(x_j,w_k)+1 )}=\\frac{Count(x_j,w_k)+1}{\\sum_{j=1}^{d=|v|} (Count(x_j,w_k))+|V|} \\ldots \\ldots (3)\n",
    "$$\n",
    "\n",
    "\n",
    "Where \n",
    "\n",
    "$Count(x_j,w_k)$: is counts of the particular word $x_j$ for class $w_k$.\n",
    "\n",
    "${\\sum_{j=1}^{d=|v|}Count(x_j,w_k))}:$ is summation of all counts of words $x_j$ for class $w_k$ for vocabulary $V$. \n",
    "\n",
    "\n",
    "At last for new document we can use equation $\\hat(B)$ to estimate posterior Probability ${P(w_k\\mid \\mathbf x_j)} $ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let’s walk through an example of training and testing naive Bayes with add-one smoothing. We’ll use a sentiment analysis domain with the two classes positive (+) and negative (-), and take the following miniature training and test documents simplified from actual movie reviews.\n",
    "\n",
    "| class($w_k $)   | Document                               |\n",
    "| ----------------| ---------------------------------------| \n",
    "| $-$             | just plain boring                      |\n",
    "| $-$             | entirely predictable and lacks energy  |\n",
    "| $-$             | no surprises and very very few laughs  |\n",
    "| $+$             | very great very powerful               |\n",
    "| $+$             | the most fun film of the summer        |\n",
    "| $+$             | great scenes great film                |\n",
    "| $-$             | no laughs\n",
    "\n",
    "The vocabulary is \n",
    "\n",
    "$$\n",
    "V=\n",
    "\\begin{bmatrix}\n",
    "W_1= just,\\ W_2= plain,\\ W_3=boring,\\ W_4= entirely,\\ W_5= predictable,\\ W_6= and,\\ W_7= lacks,\\ W_8=energy,\\ W_9= no, W_{10}= surprises,\\ W_{11}= very,\\ W_{12}= few,\\ W_{13}= laughs,\\ W_{14}= powerful, \\ W_{15}=the,\\ W_{16}= most,\\ W_{17}=fun,\\ W_{18}=film ,\\\\ W_{19}=of,\\ W_{20}= summer ,\\ W_{21}=great \\ W_{22}=scenes \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf X^{w_1=-}=\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\\\\n",
    "0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\\\ \n",
    "0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 2 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\\\\n",
    "- & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & -  \\\\\n",
    "1 & 1 & 1 & 1 & 1 & 2 & 1 & 1 & 2 & 1 & 2 & 1 & 2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf X^{w_2=+}=\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 2 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0  \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 2 & 1 & 1 & 1 & 1 & 1 & 0 & 0  \\\\ \n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 2 & 1  \\\\\n",
    "- & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & -  \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 2 & 0 & 0 & 1 & 2 & 1 & 1 & 1 & 1 & 1 & 3 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Test data \n",
    "\n",
    "| class($w_k $)   | Document                               |\n",
    "| ----------------| ---------------------------------------| \n",
    "|      ?          |predictable with no fun                 |\n",
    "\n",
    "We can write feature vector for this document as \n",
    "\n",
    "1. $\\mathbf {\\bar x_{j}} = (0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0)^{T}$\n",
    "\n",
    "\n",
    "\n",
    "The total number of documents in the training set ${N_c} =7$; $N_{w_{1}=-} =4$, $N_{w_{2}=+} =3$\n",
    "\n",
    "Using equation (2), we can estimate the prior probabilities from the training data as:\n",
    "\n",
    "$$\n",
    "P(w_1=-)=\\frac{4}{7}\\ ; \\hspace{1cm} P(w_2=+)=\\frac{3}{7}\n",
    "$$\n",
    "\n",
    "\n",
    "The word *with* doesnt occur in the training set, so we drop it completely ,we dont use unknown word models for naive Bayes. The likelihoods from the training set for the remaining three words **predictable”, no,** and\n",
    "**fun** ,are as follows, from Eq.$(3)$. \n",
    "\n",
    "$$\n",
    "P(x_{5}=predictable\\mid w_1=-)= \\frac{1+1}{17+22}=\\frac{2}{39} \\hspace{1cm} P(x_{5}=predictable\\mid w_2=+)= \\frac{0+1}{14+22}=\\frac{1}{36}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_{9}=no\\mid w_1=-)= \\frac{1+1}{17+22}=\\frac{2}{39} \\hspace{1cm} P(x_{9}=no\\mid w_2=+)= \\frac{0+1}{14+22}=\\frac{1}{36}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_{17}=fun\\mid w_1=-)= \\frac{0+1}{17+22}=\\frac{1}{39} \\hspace{1cm} P(x_{17}=fun\\mid w_2=+)= \\frac{1+1}{14+22}=\\frac{2}{36}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "{P(w_1=-\\mid \\mathbf {\\bar x_{j}})}=\\frac{2\\times2\\times1}{39^{3}}\\times \\frac{4}{7}=3.853\\times 10^{-5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{P(w_2=+\\mid \\mathbf {\\bar x_{j}})}=\\frac{1\\times1\\times2}{36^{3}}\\times \\frac{3}{7}=1.837\\times 10^{-5}\n",
    "$$\n",
    "\n",
    "The model thus predicts the class *negative* for the test sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Naive Bayes implementation for sentiment analysis  <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "I will use the data set from [Kaggle](https://www.kaggle.com/burhanykiyakoglu/predicting-sentiment-from-clothing-reviews). And I will try to Predict the Rating as range from $1$ to $5$. \n",
    "\n",
    "First we will import all the module required and then read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import nltk \n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/anil/ML/ML_1/Machine-Learning-Digital-Book-/Naive Bayes Classification/Womens Clothing E-Commerce Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Initmates</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>Intimates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Clothing ID  Age                    Title  \\\n",
       "0           0          767   33                      NaN   \n",
       "1           1         1080   34                      NaN   \n",
       "2           2         1077   60  Some major design flaws   \n",
       "3           3         1049   50         My favorite buy!   \n",
       "4           4          847   47         Flattering shirt   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  Absolutely wonderful - silky and sexy and comf...       4                1   \n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
       "2  I had such high hopes for this dress and reall...       3                0   \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "4  This shirt is very flattering to all due to th...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \n",
       "0                        0       Initmates        Intimate  Intimates  \n",
       "1                        4         General         Dresses    Dresses  \n",
       "2                        0         General         Dresses    Dresses  \n",
       "3                        0  General Petite         Bottoms      Pants  \n",
       "4                        6         General            Tops    Blouses  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[60:90]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['Review Text'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['Rating'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['Title'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_data = df[df[\"Rating\"]==2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can uncomment above lines and check yourself about the NULL values, how the review in **Title** column is related to sentiment in **Review Text**  and also with Rating. So In the both **Title** and **Review Text** *NaN* values are present Which we will replace by the Reviews by inspecting the rating for each example as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_nan = {\n",
    "    1: 'Poor Unflattering Itchy tags Bad worst sad Disappointed Ugly Terrible',\n",
    "    2: 'Not',\n",
    "    3: 'Fair flaws cheap Boring Awkward',\n",
    "    4: 'perfect pretty ',\n",
    "    5: 'Flattering Love cute fun perfect Versatile great beautiful Compliments Gorgeous'       \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Title'].isnull(), 'Title'] = df[df['Title'].isnull()]['Rating'].replace(rep_nan)\n",
    "df.loc[df['Review Text'].isnull(), 'Review Text'] = df[df['Review Text'].isnull()]['Rating'].replace(rep_nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>perfect pretty</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Initmates</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>Intimates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>Flattering Love cute fun perfect Versatile gre...</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Clothing ID  Age  \\\n",
       "0           0          767   33   \n",
       "1           1         1080   34   \n",
       "2           2         1077   60   \n",
       "3           3         1049   50   \n",
       "4           4          847   47   \n",
       "\n",
       "                                               Title  \\\n",
       "0                                    perfect pretty    \n",
       "1  Flattering Love cute fun perfect Versatile gre...   \n",
       "2                            Some major design flaws   \n",
       "3                                   My favorite buy!   \n",
       "4                                   Flattering shirt   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  Absolutely wonderful - silky and sexy and comf...       4                1   \n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
       "2  I had such high hopes for this dress and reall...       3                0   \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "4  This shirt is very flattering to all due to th...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \n",
       "0                        0       Initmates        Intimate  Intimates  \n",
       "1                        4         General         Dresses    Dresses  \n",
       "2                        0         General         Dresses    Dresses  \n",
       "3                        0  General Petite         Bottoms      Pants  \n",
       "4                        6         General            Tops    Blouses  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df[['Title','Review Text','Rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>perfect pretty</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Flattering Love cute fun perfect Versatile gre...</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                                    perfect pretty    \n",
       "1  Flattering Love cute fun perfect Versatile gre...   \n",
       "2                            Some major design flaws   \n",
       "3                                   My favorite buy!   \n",
       "4                                   Flattering shirt   \n",
       "\n",
       "                                         Review Text  Rating  \n",
       "0  Absolutely wonderful - silky and sexy and comf...       4  \n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5  \n",
       "2  I had such high hopes for this dress and reall...       3  \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5  \n",
       "4  This shirt is very flattering to all due to th...       5  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df1.iloc[:,0:2].values\n",
    "#X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will try to predict different class of rating using Naive byes so we will separate the rating as target variable $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df1.iloc[:,2:3].values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Now we perform the text preprocessing  <a class=\"anchor\" id=\"5.1\"></a>\n",
    "\n",
    "**Tokenization:**\n",
    "\n",
    "Tokenization describes the general process of breaking down a text corpus into individual elements that serve as input for various natural language processing algorithms. Usually, tokenization is accompanied by other optional processing steps, such as the removal of stop words and punctuation characters, stemming or lemmatizing, and the construction of n-grams. Below is an example of a simple but typical tokenization step that splits a sentence into individual words, removes punctuation, and converts all letters to lowercase.\n",
    "\n",
    "![](Tokanization.png)\n",
    "$$\\mathbf {Figure \\ 5: \\ Example \\ of \\ tokenization.}$$\n",
    "\n",
    "**Stop Words:**\n",
    "\n",
    "Stop words are words that are particularly common in a text corpus and thus considered as rather un-informative (e.g., words such as so, and, or, the, ...\"). One approach to stop word removal is to search against a language-specific stop word dictionary. \n",
    "\n",
    "![](stop.png)\n",
    "$$\\mathbf {Figure \\ 6: \\ Example \\ of \\ stop \\ word \\ removal.}$$\n",
    "\n",
    "**Stemming and Lemmatization:**\n",
    "\n",
    "Stemming describes the process of transforming a word into its root form. The original stemming algorithm was developed my Martin F. Porter in 1979 and is hence known as Porter stemmer [7](#r7) \n",
    "\n",
    "![](stemm.png)\n",
    "$$\\mathbf {Figure \\ 7: Example \\ of\\ Porter\\ stemming.}$$\n",
    "\n",
    "Stemming can create non-real words, such as \"thu\" in the example above.In contrast to stemming, lemmatization aims to obtain the canonical (grammatically correct) forms of the words, the so-called lemmas. Lemmatization is computationally more difficult and expensive than stemming, and in practice,both stemming and lemmatization have little impact on the performance of text classification [8](#r8)\n",
    "\n",
    "![](Lamm.png)\n",
    "$$\\mathbf {Figure \\ 8: Example \\ of\\ lemmatization.}$$\n",
    "\n",
    "\n",
    "**N-grams:**\n",
    "\n",
    "In the $n$-gram model, a token can be defined as a sequence of $n$ items. The simplest case is the so-called unigram ($1$-gram) where each word consists of exactly one word, letter, or symbol. All previous examples were unigrams so far. Choosing the optimal number n depends on the language as well as the particular application.\n",
    "\n",
    "![](n-grams.png)\n",
    "$$\\mathbf {Figure \\ 9: n-grams.}$$\n",
    "\n",
    "\n",
    "Here we are using the Python [NLTK library.](http://www.nltk.org/) You can also use `scikit-learn` [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform)and also [Text feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) for $n$-grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION = set(string.punctuation)  # Predefined set of punctuation\n",
    "STOPWORDS = set(stopwords.words('english')) # Predefined set of stop words  \n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "def tokenize_X(text):\n",
    "    tokens = word_tokenize(str(text)) # tokenization   \n",
    "    #print(tokens)\n",
    "    \n",
    "    lowercased = [t.lower() for t in tokens] # make it lower case \n",
    "    #print(lowercased)\n",
    "    no_punctuation = []\n",
    "    for word in lowercased:\n",
    "        punct_removed = ''.join([letter for letter in word if not letter in PUNCTUATION]) # Remove punctuation\n",
    "        no_punctuation.append(punct_removed)\n",
    "   \n",
    "    no_stopwords = [w for w in no_punctuation if not w in STOPWORDS] # Remove stop words\n",
    "    #print('\\n')\n",
    "    #print(no_stopwords)\n",
    "    \n",
    "    no_numbers=[w for w in no_stopwords if not w.isdigit()] # Remove the Numbers/Digits\n",
    "    #print(no_numbers)\n",
    "    #stemmed = [STEMMER.stem(w) for w in no_numbers]\n",
    "    return [w for w in no_numbers if w]\n",
    "\n",
    "tokenized_word_X = [tokenize_X(text) for text in X]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_word_X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=[]\n",
    "for i in range(len(tokenized_word_X)):\n",
    "    Data.append(TreebankWordDetokenizer().detokenize(tokenized_word_X[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T = np.unique(y, return_inverse=True)   # get the all unique values for Ratings \n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into train and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(Data, T, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Vectorization <a class=\"anchor\" id=\"5.2\"></a>\n",
    "\n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors.which we saw in module [4.2.](#4.2) and [4.3.](#4.3) which we can perform using `scikit-learn` [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform) and also [Text feature extraction.](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16440, 16133)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer #simply import CountVectorizer\n",
    "count_vect = CountVectorizer() #instantiate it's object\n",
    "X_train_counts = count_vect.fit_transform(X_train) #builds a term-document matrix ands return it\n",
    "print (X_train_counts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.Multinomial Naive Bayes Model-Sklearn  <a class=\"anchor\" id=\"5.3\"></a>\n",
    "\n",
    "The official document for [Naive Bayes scikit learn.](https://scikit-learn.org/stable/modules/naive_bayes.html) and link for [sklearn.naive_bayes.MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #importing the Sklearn's NB Fucntionality\n",
    "clf = MultinomialNB() #simply instantiate a Multinomial Naive Bayes object\n",
    "clf.fit(X_train_counts, y_train)  #calling the fit method trains it\n",
    "print (\"Training Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7046, 16133)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_counts = count_vect.transform(X_test) #builds a term-document matrix ands return it\n",
    "np.shape(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy :  0.6755606017598638\n"
     ]
    }
   ],
   "source": [
    "predicted=clf.predict(X_test_counts)\n",
    "print (\"Test Set Accuracy : \",np.sum(predicted==y_test)/float(len(predicted))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.Bernoulli Naive Bayes Model -Sklearn <a class=\"anchor\" id=\"5.4\"></a>\n",
    "\n",
    "This is the link for [sklearn.naive_bayes.BernoulliNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB(0.5) #simply instantiate a Multinomial Naive Bayes object\n",
    "clf.fit(X_train_counts, y_train)  #calling the fit method trains it\n",
    "print (\"Training Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7046, 16133)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_counts_1 = count_vect.transform(X_test) #builds a term-document matrix ands return it\n",
    "np.shape(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy :  0.6501561169457848\n"
     ]
    }
   ],
   "source": [
    "predicted=clf.predict(X_test_counts_1)\n",
    "print (\"Test Set Accuracy : \",np.sum(predicted==y_test)/float(len(predicted))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Performances of the Multi-variate Bernoulli and Multinomial Model <a class=\"anchor\" id=\"5.5\"></a>\n",
    "\n",
    "Empirical comparisons provide evidence that the multinomial model tends to outperform the multi-variate Bernoulli model if the vocabulary size is relatively large [9](#r9). However, the performance of machine learning algorithms is highly dependent on the appropriate choice of features. In the case of naive Bayes classifiers and text classification, large differences in performance can be attributed to the choices of stop word removal, stemming, and token-length [10](#r10). In practice, it is recommended that the choice between a multi-variate Bernoulli or multinomial model for text classification should precede comparative studies including different combinations of feature extraction and selection steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References: <a class=\"anchor\" id=\"r\"></a>\n",
    "\n",
    "[1] Irina Rish. An empirical study of the naive bayes classifier. In IJCAI 2001 workshop on empirical methods in artificial intelligence, pages 41-46, 2001.<a class=\"anchor\" id=\"r1\"></a>\n",
    "\n",
    "[2] Pedro Domingos and Michael Pazzani. On the optimality of the simple bayesian classifier under zero-one loss. Machine learning, 29(2-3):103-130, 1997<a class=\"anchor\" id=\"r2\"></a>\n",
    "\n",
    "[3] Joanna Kazmierska and Julian Malicki. Application of the na¨ıve bayesian classifier to optimize treatment decisions. Radiotherapy and Oncology, 86(2):211-216, 2008.<a class=\"anchor\" id=\"r3\"></a>\n",
    "\n",
    "[4] Qiong Wang, George M Garrity, James M Tiedje, and James R Cole. Naive bayesian classifier for rapid assignment of rrna sequences into the new bacterial taxonomy. Applied and environmental microbiology, 73(16):5261-5267,\n",
    "2007.<a class=\"anchor\" id=\"r4\"></a>\n",
    "\n",
    "[5] Mehran Sahami, Susan Dumais, David Heckerman, and Eric Horvitz. A bayesian approach to filtering junk e-mail. In Learning for Text Categorization: Papers from the 1998 workshop, volume 62, pages 98-105, 1998.<a class=\"anchor\" id=\"r5\"></a>\n",
    "\n",
    "[6] Harry Zhang. The optimality of naive bayes. AA, 1(2):3, 2004.<a class=\"anchor\" id=\"r6\"></a>\n",
    "\n",
    "[7] Martin F Porter. An algorithm for suffix stripping. Program: electronic library and information systems, 14(3):130-137, 1980.<a class=\"anchor\" id=\"r7\"></a>\n",
    "\n",
    "[8] Michal Toman, Roman Tesar, and Karel Jezek. Influence of word normalization on text classification. Proceedings of InSciT, pages 354-358, 2006 <a class=\"anchor\" id=\"r8\"></a>\n",
    "\n",
    "[9] Andrew McCallum, Kamal Nigam, et al. A comparison of event models for naive bayes text classification. In AAAI-98 workshop on learning for text categorization, volume 752, pages 41-48. Citeseer, 1998.\n",
    "<a class=\"anchor\" id=\"r9\"></a>\n",
    "\n",
    "[10] Lawrence M Rudner and Tahung Liang. Automated essay scoring using bayes’ theorem. The Journal of Technology, Learning and Assessment, 1(2), 2002.<a class=\"anchor\" id=\"r10\"></a>\n",
    "\n",
    "* [11] [Naive Bayes and Text Classification-Oct 4, 2014 by Sebastian Raschka](https://sebastianraschka.com/Articles/2014_naive_bayes_1.html#%E2%80%93-introduction-and-theory)\n",
    "\n",
    "* [12] [Maximum Likelihood Estimator-Practical Data Science](http://www.datasciencecourse.org/notes/mle/)\n",
    "\n",
    "* [13] [The Naive Bayes Model, Maximum-Likelihood Estimation, and the EM Algorithm Michael Collins](http://www.cs.columbia.edu/~mcollins/em.pdf) \n",
    "\n",
    "* [14] [Tuesday, December 27, 2016 Bernoulli vs Binomial vs Multinoulli vs Multinomial distributions](https://geekyisawesome.blogspot.com/2016/12/bernoulli-vs-binomial-vs-multinoulli-vs.html)\n",
    "\n",
    "* [15] [Naive Bayes and Sentiment Classification-Daniel Jurafsky and James H.Martin (Stanford University and University of Colorado at Boulder)](https://web.stanford.edu/~jurafsky/slp3/4.pdf)\n",
    "\n",
    "* [16] [Text Classification using Naive Bayes -Hiroshi Shimodaira](https://www.inf.ed.ac.uk/teaching/courses/inf2b/learnnotes/inf2b-learn-note07-2up.pdf)\n",
    "\n",
    "* [17] [Predicting Sentiment from Clothing Reviews Burhan Y. Kiyakoglu](https://www.kaggle.com/burhanykiyakoglu/predicting-sentiment-from-clothing-reviews)\n",
    "\n",
    "* [18] [The Final Act — Just Like a Naïve Bayes Pro!-Aisha Javed](https://towardsdatascience.com/the-final-act-just-like-a-na%C3%AFve-bayes-pro-5c440b511b8d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
