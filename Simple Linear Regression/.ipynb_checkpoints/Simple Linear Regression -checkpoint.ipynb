{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Content :\n",
    " [Simple Linear Regression: ](#first-bullet)\n",
    "1. [1.Linear Regression with one variable: Cost Function.](#second-bullet)\n",
    "2. [2.The cost function intuition.](#third-bullet)\n",
    "3. [3.Linear regression with one variable.](#fourth-bullet)\n",
    "4. [4.Gradient Descent.](#fifth-bullet)\n",
    "5. [5.Gradient descent algorithm for linear regression.](#sixth-bullet)\n",
    "6. [6.Implementation of the gradient descent on the plotted Data.](#seventh-bullet)\n",
    "7. [7.Fit the line with the help of the estimated parameters ${\\theta}_i$.](#eighth-bullet)\n",
    "8. [8.Visualizing $J({\\theta}_0,{\\theta}_1)$ vs ${\\theta}_0,{\\theta}_1$.](#ninth-bullet)\n",
    "9. [9.Linear Regression using scikit-learn.](#tenth-bullet)\n",
    "10. [10.Linear Regression from Statistical point of view and Least Square.](#eleventh-bullet)\n",
    "11. [11.Linear Regression using normal equations.](#11) \n",
    "\n",
    " [References](#twelfth-bullet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression:  <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "\n",
    "\n",
    "First the simple Linear regression is supervised learning method.**Supervised learing:** In which the trainig data comprises examples of the input vectors along with their corresponding target vectors.Simple linear regression is useful for finding relationship between two continuous variables or real values.One is predictor or independent variable and other is response or dependent variable. **It looks for statistical relationship but not deterministic relationship.** Relationship between two variables is said to be deterministic if one variable can be accurately expressed by the other. For example, using temperature in degree Celsius it is possible to accurately predict Fahrenheit. Statistical relationship is not accurate in determining relationship between two variables. For example, relationship between height and weight.\n",
    "\n",
    "The simplest example is you had given the size of the houses in square feet and prices (USD) of each house and you want to predict the price of the house for future input of the size.As shown below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"/home/anil/ML/ML_1/Machine-Learning-Digital-Book-/Simple Linear Regression/Img/lin-reg.png\",width=600,height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core idea is to obtain a line that best fits the data. The best fit line is the one for which total prediction error (all data points) are as small as possible. Error is the distance between the point to the regression line.\n",
    "(Do not worry if this is not clear to you as we move ahead you will get it).so we will go step by step to build the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Linear Regression with one variable: Cost Function.  <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "\n",
    "First we talk about the **hypothesis**.Do not worry about the name hypothesis it is just a fancy name.   \n",
    "\n",
    "hypothesis:\n",
    "$$\n",
    "\\begin{aligned}\\ h_{\\theta}(x)={\\theta}_0 +{\\theta}_1x \\end{aligned}\n",
    "$$ \n",
    "\n",
    "\n",
    "where $\\theta_i$ is parameters.\n",
    "\n",
    "how to choose $\\theta_i$ ?\n",
    "\n",
    "At first glance the above equation you will find similar to the straight-line equation $ y=mx+c $. Yes, your intuition is correct.$ y $ is the linear function of the variable $ x $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "\n",
    "def my_plot(theta_0,theta_1):\n",
    "    x = np.linspace(1,5,100)\n",
    "    \n",
    "    h_theta_x= theta_0 + (theta_1*x)\n",
    "    plt.figure(num=0, figsize=(6, 4), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.plot(x, h_theta_x, '-r')\n",
    "    plt.title('straight line fit with %.2f slope and %.2f as a y intercept.' % (theta_1,theta_0) ,fontsize=12)\n",
    "    plt.xlabel('$x$',fontsize=14)\n",
    "    plt.ylabel(r'$\\ h_{\\theta}(x)$ ',fontsize=14)\n",
    "    plt.grid(which='both')\n",
    "    plt.show()\n",
    "\n",
    "#my_plot(theta_0=(1),theta_1=(1))\n",
    "    \n",
    "# create a slider\n",
    "interact(my_plot, theta_0=(0,2,0.5),theta_1=(0,1,0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plots we had use different values of $\\theta_0 $ and $\\theta_1 $ and plot the different hypothesis.\n",
    "\n",
    "In linear regression, we are fitting the straight line through data as mention in the house prediction example.We chose $\\theta_0 $ and $\\theta_1$ such that my hypothesis $ h_{\\theta}(x)$ the **value we predict on input vector $ X $** is very close to the actual (label/target/answers) values $ y $ given in the training examples $ (x,y)$.In simple terms given $ x $ values(house sizes) and values for $ y $ (Price of the houses) by choosing the plausible values for $\\theta$ we try to predict the accurate value of $ y $ to given new value for $ x $ \n",
    "\n",
    "So let's understand mathematically how we can formulate this idea using the **Cost Function**.\n",
    "\n",
    "$$\n",
    "\\min_{{\\theta}_0,{\\theta}_1} \\frac{1}{2m} \\sum_{i=1}^m(h_{\\theta}(x^{(i)} )-y^{(i)})^2 \n",
    "$$\n",
    "\n",
    "Where \n",
    "$$\n",
    "\\begin{aligned}\\ h_{\\theta}(x^{(i)})={\\theta}_0 +{\\theta}_1x^{(i)} \\end{aligned}\n",
    "$$ \n",
    "\n",
    "Let's \n",
    "$$  \n",
    "J({\\theta}_0,{\\theta}_1) = \\frac{1}{2m} \\sum_{i=1}^m(h_{\\theta}(x^{(i)} )-y^{(i)})^2 \n",
    "$$ \n",
    "\n",
    "$$ \n",
    "\\min_{{\\theta}_0,{\\theta}_1} J({\\theta}_0,{\\theta}_1)\n",
    "$$\n",
    "\n",
    "Where\n",
    "$\n",
    "J({\\theta}_0,{\\theta}_1)\n",
    "$\n",
    "is the **Cost Function**\n",
    "\n",
    "In the cost function the superscript $ i $ is running from $ 1 $ to $ m $ for my $ m $ training examples.The minimization problem says to find the values of parameters $\\theta_0 $ and $\\theta_1$ so that the average value of the **squared error** between predicted value(hypothesis)$ h_{\\theta}(x)$ and or minus the actual value $ y $  is as small as possible.The cost function is also known as square error cost function which is our objective function.\n",
    "\n",
    "Think about this:\n",
    "1. Why one over 2 \n",
    "2. Why squared error cost function? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.The cost function intuition. <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "\n",
    "Now we will try to understand the cost function more intuitively.So following is the task.\n",
    "\n",
    "hypothesis:\n",
    "$\n",
    "\\begin{aligned}\\ h_{\\theta}(x)={\\theta}_0 +{\\theta}_1x \\end{aligned}\n",
    "$ \n",
    "\n",
    "Parameters:\n",
    "$\n",
    "{\\theta}_0,{\\theta}_1 \n",
    "$\n",
    "\n",
    "Cost function:\n",
    "$  \n",
    "J({\\theta}_0,{\\theta}_1) = \\frac{1}{2m} \\sum_{i=1}^m(h_{\\theta}(x^{(i)} )-y^{(i)})^2 \n",
    "$ \n",
    "\n",
    "Goal:\n",
    "$$ \n",
    "\\min_{{\\theta}_0,{\\theta}_1} J({\\theta}_0,{\\theta}_1)\n",
    "$$\n",
    "\n",
    "So goal here is to minimize the cost function $J({\\theta}_0,{\\theta}_1)$,Consider the simplified case i.e. ${\\theta}_0=0$  \n",
    "$\n",
    "\\begin{aligned}\\ h_{\\theta}(x)={\\theta}_1x \\end{aligned}\n",
    "$ \n",
    "\n",
    "This will always the straight line passing from the origin.The cost function will be \n",
    "$  \n",
    "J({\\theta}_1) = \\frac{1}{2m} \\sum_{i=1}^m(h_{\\theta}(x^{(i)} )-y^{(i)})^2 \n",
    "$\n",
    "and \n",
    "\n",
    "$$ \n",
    "\\min_{{\\theta}_1} J({\\theta}_1)\n",
    "$$\n",
    "\n",
    "Now we will visualize how for different values of (fixed ${\\theta}_1$) how the hypothesis is change which is the function of the $x$ and same time how the cost function (which is the function of the ${\\theta}_1$ ) changes for those values of ${\\theta}_1$.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "\n",
    "def my_plot_1(theta_1):\n",
    "    theta_0=0\n",
    "    print(theta_1)\n",
    "    X= np.linspace(1,5,10)\n",
    "    Y= np.linspace(1,5,10)\n",
    "    h_theta_x= theta_0 + (theta_1*X)\n",
    "    # plot of hypothesis vs X\n",
    "    plt.figure(num=0, figsize=(6, 4), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.plot(X,Y,'ro',X,h_theta_x,'k')\n",
    "    plt.title(r'$\\ h_{\\theta}(x)$ (for fixed $\\theta_1$,$\\ h_{\\theta}(x)$ is a function of $x$)',fontsize=12)\n",
    "    plt.xlabel('$X$',fontsize=12)\n",
    "    plt.ylabel('$Y$',fontsize=12)\n",
    "    plt.grid(which='both')\n",
    "    plt.show()\n",
    "    # estimating the cost function and ploting cost function vs theta plot\n",
    "    j_theta_1=(1/2)*mean_squared_error(Y,h_theta_x)\n",
    "    print(j_theta_1)\n",
    "\n",
    "interactive_plot=interact(my_plot_1,theta_1=(0,2,0.125))\n",
    "output = interactive_plot.widget.children[-1]\n",
    "output.layout.height = '350px'\n",
    "interactive_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as ipyw\n",
    "\n",
    "j_theta_1T=[5.3148,4.0691,2.9895,2.076099,1.3287,0.74739,0.3321,0.08304,0,0.08304,0.3321,0.7473,1.3287,2.076099,2.9895,4.0691,5.3148]\n",
    "X_T=np.linspace(0,2,17)\n",
    "\n",
    "def my_plot_2(t):\n",
    "    print(t)\n",
    "    plt.figure(num=0, figsize=(6, 4), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.plot(X_T,j_theta_1T,'b',\n",
    "             t/8,j_theta_1T[t],'ro')\n",
    "    plt.title(r'$J({\\theta}_1)$ (function of the parameter $\\theta_1)$',fontsize=12)\n",
    "    plt.xlabel(r'$\\theta_1$',fontsize=12)\n",
    "    plt.ylabel(r'$J({\\theta}_1)$',fontsize=12)\n",
    "    plt.grid(which='both')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interactive_plot=ipyw.interact(\n",
    "    my_plot_2,\n",
    "    t=ipyw.IntSlider(min=0, \n",
    "                     max=(len(j_theta_1T)-1), \n",
    "                     step=1, \n",
    "                     value=0)\n",
    ")\n",
    "output = interactive_plot.widget.children[-1]\n",
    "output.layout.height = '350px'\n",
    "interactive_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above first plot the hypothesis\n",
    "$\n",
    "\\begin{aligned}\\ h_{\\theta}(x)={\\theta}_1x \\end{aligned}\n",
    "$ \n",
    "for the fixed value of the $ {\\theta}_1 $ is function of $x$. You can change the value of the $ {\\theta}_1 $ from the slider and see for the different fixed values of the ${\\theta}_1 $ line changes the slope and at ${\\theta}_1 =1 $ it fits the data perfectly.you can check as the value (Printed above the line fit) of the ${\\theta}_1 $ changes the from $0$ to $2$ the mean square error get decrease and again increases as (Printed below the line fit) \n",
    "\n",
    "second plot is the plot of $J({\\theta}_1)$ vs ${\\theta}_1 $ & how the cost function $J({\\theta}_1)$ get minimize as we fit the line perfectly to the data at ${\\theta}_1 =1 $ and you can verify that as line is perfectly fit at  ${\\theta}_1 =1 $  the $J({\\theta}_1)=0$ and it is visible in above plot just change the `t` which is index for different ${\\theta}_1 $ and  at $t=8$ the ${\\theta}_1 =1 $ and $J({\\theta}_1)=0$.   \n",
    "\n",
    "Think about this:\n",
    "\n",
    "1. What if we consider case ${\\theta}_0$ included in the hypothesis equation?\n",
    "\n",
    "2. What would be the dimension of the plot Hypothesis vs Parameters i.e.\n",
    "$\n",
    "\\begin{aligned}\\ h_{\\theta}(x)={\\theta}_0 +{\\theta}_1x \\end{aligned}\n",
    "$ \n",
    "   vs ${\\theta}_0 ,{\\theta}_1 $ ?\n",
    "\n",
    "3. Does the shape of the plot remain same as bowl shape or changes ?\n",
    "\n",
    "4. What is the relevant way to reach the minimum point or minimize the cost function to best fit the hypothesis?\n",
    "\n",
    "To answer these questions we take a real time data and work on it here onwards.Before going ahead one last thing I would like to mention is about **Least Square error**. If you carefully see the cost function $J({\\theta})$ which we are trying to minimize to find ${\\theta}$ is actually an **error value** or **average square error** which we want as least value,hence called **Least Square error**. We will talk more about this in module 10. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Linear regression with one variable. <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "\n",
    "In this part of this,we will implement linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities.\n",
    "\n",
    "## Plotting the Data\n",
    "\n",
    "Before starting on any task, it is often useful to understand the data by visualizing it. For this dataset, you can use a scatter plot to visualize the data, since it has only two properties to plot (profit and population). (Many other problems that you will encounter in real life are multi-dimensional and can not  be plotted on a 2-d plot.)\n",
    "\n",
    "The file `ex1data1.txt` contains the dataset for our linear regression prob- lem. The first column is the population of a city and the second column is the profit of a food truck in that city. A negative value for profit indicates a loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#read comma separated data and give the proper names to the columnes\n",
    "df = pd.read_csv('/home/anil/ML/ML_1/Machine-Learning-Digital-Book-/Simple Linear Regression/ex1data1.txt',header = None,names=[\"Population\",\"Profit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # first five rows along with columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # good to know the dimension  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.iloc[:,0:1].values\n",
    "#x=df1\n",
    "#print(x)\n",
    "y=df.iloc[:,1:2].values\n",
    "#y=df2\n",
    "#print(len(y))\n",
    "plt.figure(num=0, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(x,y,'ro',markersize=4,label=\"Training Data\");     # plot the data \n",
    "plt.ylabel('Profit in $10,000s',fontsize=12);             # Set the y axis label\n",
    "plt.xlabel('Population of City in 10,000s',fontsize=12);  # Set the x axis label\n",
    "\n",
    "# Turn on the minor TICKS, which are required for the minor GRID\n",
    "plt.minorticks_on()\n",
    "\n",
    "# Customize the major grid\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='blue')\n",
    "# Customize the minor grid\n",
    "\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Gradient Descent. <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "\n",
    "So answer for the **fourth question** above is as follows,We need a methodology or the Algorithm which will automatically find the values of the parameters ${\\theta}_0 ,{\\theta}_1 $ for my hypothesis\n",
    "$\n",
    "\\begin{aligned}\\ h_{\\theta}(x)={\\theta}_0 +{\\theta}_1x \\end{aligned}\n",
    "$\n",
    "and reduced my cost function $J({\\theta}_0,{\\theta}_1)$ to the minimum value by adjusting the ${\\theta}_0 ,{\\theta}_1 $.The name of that algorithm is **Gradient Descent**.The Gradient Descent is very generalize algorithm to reduce/minimize any Non linear function,here we will use to minimizing the cost function\n",
    "$\n",
    "J({\\theta}_0,{\\theta}_1)\n",
    "$ \n",
    "\n",
    "Before going ahead specifically to use the Gradient Descent for above plotted data set for univariate linear regression, we first understand the generalize overview of the algorithm.\n",
    "\n",
    "So supposed we have function $J({\\theta}_0,{\\theta}_1,{\\theta}_2,{\\theta}_3 ..{\\theta}_n)$ and wanted to minimize it $$ \n",
    "\\min_{{\\theta}_0,{\\theta}_1,{\\theta}_n} J({\\theta}_0,{\\theta}_1...{\\theta}_n)\n",
    "$$\n",
    "\n",
    "**Outline**: consider the case of the two parameters $ {\\theta}_0,{\\theta}_1$ \n",
    "\n",
    "* start with some arbitrary value $ {\\theta}_0,{\\theta}_1 $\n",
    "\n",
    "* keep changing the ${\\theta}_0,{\\theta}_1 $ to reduce $J({\\theta}_0,{\\theta}_1)$ until we end up at the minimum position or may be local minimum.\n",
    "\n",
    "As shown in the below figure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"/home/anil/ML/ML_1/Machine-Learning-Digital-Book-/Simple Linear Regression/Img/G1.png\",width=600,height=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the above figure/plot we found out the answer for **first Question** and the **Second Question** as we include ${\\theta}_0$ in the hypothesis\n",
    "$\n",
    "\\begin{aligned}\\ h_{\\theta}(x)={\\theta}_0 +{\\theta}_1x \\end{aligned}\n",
    "$\n",
    "The cost function will reach to minimum values by varying the both ${\\theta}_0,{\\theta}_1 $ i.e. the cost function is function of the both ${\\theta}_0,{\\theta}_1$. The plot of the\n",
    "$\n",
    "J({\\theta}_0,{\\theta}_1)\n",
    "$\n",
    "vs\n",
    "${\\theta}_0,{\\theta}_1$. is 2-D.\n",
    "\n",
    "Here we started with an arbitrary value $ {\\theta}_0,{\\theta}_1 $ values and with step forward of steepest descent we reach to the local minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"/home/anil/ML/ML_1/Machine-Learning-Digital-Book-/Simple Linear Regression/Img/G2.png\",width=600,height=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent has interesting property as shown above as we start with different arbitrary value \n",
    "$ {\\theta}_0,{\\theta}_1 $ we end up with an another local optimum.\n",
    "\n",
    "Think about this: \n",
    "1.  Does the gradient descent always converge to an optimum (global or local)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Gradient descent algorithm**\t\n",
    "\n",
    "That is all about the intuition in the pictorial form,let's look at the mathematically the same\n",
    "\n",
    "repeat until  converges \n",
    "{\n",
    " \n",
    "$\n",
    "\\large {\\theta}_j:={\\theta}_j- \\Large\\alpha\\frac{\\partial J({\\theta}_0,{\\theta}_1)}{\\partial {\\theta}_j} \n",
    "$\n",
    "\n",
    "( for $\\large j=0$ and $\\large j=1$ )\n",
    "\n",
    "}\n",
    "\n",
    "  \n",
    "simultaneously update the the ${\\theta}_0 $ and ${\\theta}_1$\n",
    "\n",
    "$\n",
    "\\large {\\theta}_0:={\\theta}_0- \\Large\\alpha\\frac{\\partial J({\\theta}_0,{\\theta}_1)}{\\partial {\\theta}_0}\n",
    "$\n",
    "\n",
    "$\n",
    "\\large {\\theta}_1:={\\theta}_1- \\Large\\alpha\\frac{\\partial J({\\theta}_0,{\\theta}_1)}{\\partial {\\theta}_1}\n",
    "$\n",
    "\n",
    "\n",
    "Here the $\\large\\alpha$ is the learning rate and soon we will talk about this,second thing $\\large \\frac{\\partial J({\\theta}_0,{\\theta}_1)}{\\partial {\\theta}_j}$ is the first order partial differentiation which you can think like rate of change of the cost function $ J({\\theta}_0,{\\theta}_1)$ w.r.t  ${\\theta}_j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition of the learning rate $\\large\\alpha$ and partial differentiation $\\large \\frac{\\partial J({\\theta}_0,{\\theta}_1)}{\\partial {\\theta}_j}$**\n",
    "\n",
    "To understand the learning rate $\\large\\alpha$  and partial differentiation $\\large \\frac{\\partial J({\\theta}_0,{\\theta}_1)}{\\partial {\\theta}_j}$ we will take again the simple case as(We had taken while understanding the cost function intuition )by considering only ${\\theta}_1$ any real number,so the hypothesis become\n",
    "$\n",
    "\\begin{aligned}\\ h_{\\theta}(x)={\\theta}_1x \\end{aligned}\n",
    "$\n",
    "and try to minimize \n",
    "$$\n",
    "\\min_{{\\theta}_1} J({\\theta}_1)\n",
    "$$\n",
    "\n",
    "The Gradient descent algorithm equations become \n",
    "$\n",
    "\\large {\\theta}_1:={\\theta}_1- \\Large\\alpha\\frac{\\partial J({\\theta}_1)}{\\partial {\\theta}_1}\n",
    "$\n",
    "\n",
    "Here the $\\large\\alpha$ is the learning rate which is always a positive number & decides the step size/width.The steps/width with which we move towards the local/global minimum or reduce the cost function to minimum value.The partial differentiation $\\large \\frac{\\partial J({\\theta}_1)}{\\partial {\\theta}_1}$ is **slope to the curve** of\n",
    "the plot of the\n",
    "$\n",
    "J({\\theta}_1)\n",
    "$\n",
    "vs\n",
    "${\\theta}_1$.\n",
    "The slope can be positive or negative and as $\\large\\alpha$ is always positive,**hence the parameter ${\\theta}_1$ updated positively or negatively with the steps/step size/width depends upon the $\\large\\alpha$.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"/home/anil/ML/ML_1/Machine-Learning-Digital-Book-/Simple Linear Regression/Img/G3.png\",width=600,height=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above fig shows the effect of the large and small value learning parameter $\\large\\alpha$ on search of the minimum value of the cost function $J({\\theta}_1)$ or search of the local optimum or to zero slope value.When  $\\large\\alpha$ is small the Gradient descent is slow will take time to converge and when $\\large\\alpha$ is large Gradient descent overshoot the minimum and can fail to converge.\n",
    "\n",
    "Now the answer to the question **Does the gradient descent always converge to an optimum (global or local)?**\n",
    "Gradient Descent is an algorithm which is designed to find the optimal points, but these optimal points are not necessarily global. And yes if it happens that it diverges from a local location it may converge to another optimal point but its probability is not too much. The reason is that the step size might be too large that prompts it recede one optimal point and the probability that it oscillates is much more than convergence.\n",
    "\n",
    "What if initial selection of ${\\theta}_1$ is itself land us on local optimum then gradient descent will not update the ${\\theta}_1$ as the slope $\\large \\frac{\\partial J({\\theta}_1)}{\\partial {\\theta}_1}$ at value of ${\\theta}_1$ to the curve is zero as shown below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"/home/anil/ML/ML_1/Machine-Learning-Digital-Book-/Simple Linear Regression/Img/G.png\",width=600,height=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly gradient descent can converges to local optimum even when  $\\large\\alpha$  is fixed. As  $\\large\\alpha$  is fixed and as we approach to the local minimum gradient descent will automatically take smaller steps,no need to decrease the $\\large\\alpha$  over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Gradient descent algorithm for linear regression. <a class=\"anchor\" id=\"sixth-bullet\"></a>\t\n",
    "\n",
    "**Gradient descent algorithm**\t\n",
    "\n",
    "That is all about the intuition in the pictorial form,let's look at the mathematically the same\n",
    "\n",
    "repeat until  converges \n",
    "{\n",
    " \n",
    "$\n",
    "\\Large {\\theta}_j:={\\theta}_j- \\Large\\alpha\\frac{\\partial J({\\theta}_0,{\\theta}_1)}{\\partial {\\theta}_j} \n",
    "$\n",
    "\n",
    "( for $\\large j=0$ and $\\large j=1$ )\n",
    "\n",
    "}\n",
    "\n",
    "**Linear regression model**\n",
    "\n",
    "hypothesis:\n",
    "$\n",
    "\\begin{aligned}\\Large\\ h_{\\theta}(x)={\\theta}_0 +{\\theta}_1x \\end{aligned}\n",
    "$ \n",
    "\n",
    "\n",
    "Cost function:\n",
    "$  \n",
    "\\Large\\ J({\\theta}_0,{\\theta}_1) = \\frac{1}{2m} \\sum_{i=1}^m(h_{\\theta}(x^{(i)} )-y^{(i)})^2 \n",
    "$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "\\Large\\frac{\\partial J({\\theta}_0,{\\theta}_1)}{\\partial {\\theta}_j}=\n",
    "\\Large\\frac{\\partial \\frac{1}{2m} \\sum_{i=1}^m(h_{\\theta}(x^{(i)} )-y^{(i)})^2 }{\\partial {\\theta}_j}=\n",
    "\\Large\\frac{\\partial \\frac{1}{2m} \\sum_{i=1}^m({\\theta}_0 +{\\theta}_1x^{(i)}-y^{(i)})^2 }{\\partial {\\theta}_j}\n",
    "$\n",
    "\n",
    "$\n",
    "j=0 :\\large\\frac{\\partial J({\\theta}_0,{\\theta}_1)}{\\partial {\\theta}_0}=\\frac{1}{m} \\sum_{i=1}^m(h_{\\theta}(x^{(i)} )-y^{(i)})\n",
    "$\n",
    "\n",
    "$\n",
    "j=1 :\\large\\frac{\\partial J({\\theta}_0,{\\theta}_1)}{\\partial {\\theta}_1}=\\frac{1}{m} \\sum_{i=1}^m(h_{\\theta}(x^{(i)} )-y^{(i)})(x^{(i)}) \n",
    "$\n",
    "\n",
    "**Gradient descent algorithm for Linear Regression**\n",
    "\n",
    "simultaneously update the the ${\\theta}_0 $ and ${\\theta}_1$\n",
    "\n",
    "repeat until  converges \n",
    "{\n",
    "\n",
    "$\n",
    "\\large {\\theta}_0:={\\theta}_0- \\Large\\alpha\\frac{\\partial J({\\theta}_0,{\\theta}_1)}{\\partial {\\theta}_0}\n",
    "$\n",
    "\n",
    "$\n",
    "\\large {\\theta}_1:={\\theta}_1- \\Large\\alpha\\frac{\\partial J({\\theta}_0,{\\theta}_1)}{\\partial {\\theta}_1}\n",
    "$\n",
    "\n",
    "}\n",
    "\n",
    "Which becomes\n",
    "\n",
    "simultaneously update the the ${\\theta}_0 $ and ${\\theta}_1$\n",
    "\n",
    "repeat until  converges \n",
    "{\n",
    "\n",
    "$\n",
    "\\large {\\theta}_0:={\\theta}_0- \\Large\\alpha\\frac{1}{m} \\sum_{i=1}^m(h_{\\theta}(x^{(i)} )-y^{(i)})\n",
    "$\n",
    "\n",
    "$\n",
    "\\large {\\theta}_1:={\\theta}_1- \\Large\\alpha\\frac{1}{m} \\sum_{i=1}^m(h_{\\theta}(x^{(i)} )-y^{(i)})(x^{(i)}) \n",
    "$\n",
    "\n",
    "}\n",
    "\n",
    "As we discussed above the we start with arbitrary value of ${\\theta}_0,{\\theta}_1$ at the top of the hill at two different location and end up with two different local optimum. here for cost function of the linear regression by considering ${\\theta}_0$ the shape of The plot of the\n",
    "$\n",
    "J({\\theta}_0,{\\theta}_1)\n",
    "$\n",
    "vs\n",
    "${\\theta}_0,{\\theta}_1$. is 2-D Bowl shape i.e.the cost function\n",
    "$\n",
    "J({\\theta}_0,{\\theta}_1)\n",
    "$\n",
    "is convex function as shown below.Which has only one **global optimum/minimum**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"/home/anil/ML/ML_1/Machine-Learning-Digital-Book-/Simple Linear Regression/Img/G4.png\",width=600,height=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Implementation of the gradient descent on the above plotted Data. <a class=\"anchor\" id=\"seventh-bullet\"></a>\n",
    "\n",
    "Now we will fit the linear regression parameters θ to our dataset using gradient descent\n",
    "**Update Equations**\n",
    "The objective of linear regression is to minimize the cost function\n",
    "\n",
    "$$  \n",
    "J({\\theta}_0,{\\theta}_1) = \\frac{1}{2m} \\sum_{i=1}^m(h_{\\theta}(x^{(i)} )-y^{(i)})^2 \n",
    "$$ \n",
    "\n",
    "where the hypothesis  $ h_{\\theta}(x)$  is given by the linear model\n",
    "$$\n",
    "h_{\\theta}(x)={\\theta}^Tx = {\\theta}_0 + {\\theta}_1x_1 \n",
    "$$\n",
    "Recall that the parameters of your model are the ${\\theta}_j$ values. These are the values you will adjust to minimize cost $J({\\theta}_0,{\\theta}_1)$. One way to do this is to use the batch gradient descent algorithm. In batch gradient descent, each iteration performs the update\n",
    "\n",
    "$$\n",
    "{\\theta}_j:={\\theta}_j- \\alpha\\frac{1}{m} \\sum_{i=1}^m(h_{\\theta}(x^{(i)} )-y^{(i)})(x_j^{(i)}) \n",
    "$$\n",
    "\n",
    "(simultaneously update ${\\theta}_j$ for all $j$).\n",
    "\n",
    "With each step of gradient descent, your parameters ${\\theta}_j$ come closer to the optimal values that will achieve the lowest cost $J({\\theta}_0,{\\theta}_1)$ \n",
    "\n",
    "we have already set up the data for linear regression. In the following lines, we add another dimension to our data to accommodate the ${\\theta}_0 $ intercept term.i.e. our hypothesis become\n",
    "$$\n",
    "h_{\\theta}(x)={\\theta}^Tx = {\\theta}_0x_0 + {\\theta}_1x_1 \n",
    "$$\n",
    "\n",
    "where $x_0 = 1$ always, so which is same as previous hypothesis.\n",
    "We also initialize the initial parameters to 0 and the learning rate alpha to 0.01.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.insert(x,0,1,axis=1) #Add a column of ones to x\n",
    "np.shape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta=np.zeros([2,1])  # initialize fitting parameters\n",
    "theta\n",
    "#np.shape(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and display initial cost\n",
    "\n",
    "def Compute_Cost(x,y,theta):\n",
    "    m=len(y)\n",
    "    J=0;\n",
    "    for i in range(m):\n",
    "        #print(i)\n",
    "        J=J+ (np.dot(x[i,:],(theta))-y[i])**2\n",
    "    return (J/(2*len(y)))\n",
    "\n",
    "J=Compute_Cost(x,y,theta)\n",
    "\n",
    "print('With theta = [0 ; 0] Cost computed = %.3f' %(J));\n",
    "print('Expected cost value (approx) %.2f'% 32.07);\n",
    "\n",
    "# further testing of the cost function\n",
    "\n",
    "J=Compute_Cost(x,y,[[-1],[2]])\n",
    "print('\\nWith theta = [-1 ; 2] Cost computed = %.3f' %(J) );\n",
    "print('Expected cost value (approx)%.2f' %54.24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement gradient descent as the code follow next.As you are going through the  program, make sure you understand what you are trying to optimize and what is being updated. Keep in mind that the cost J(θ) is\n",
    "parameterized by the vector ${\\theta}$, not $X$ and $y$. That is, we minimize the value of $J({\\theta}_0,{\\theta}_1)$  by changing the values of the vector ${\\theta}$, not by changing $X$ or $y$. Refer to the\n",
    "equations above.\n",
    "\n",
    "A good way to verify that gradient descent is working correctly is to look at the value of $J({\\theta}_0,{\\theta}_1)$  and check that it is decreasing with each step.The code for gradient Descent  calls `compute_Cost` on every iteration and prints the cost. Assuming you have implemented gradient descent and `compute_Cost` correctly, your value of $J({\\theta}_0,{\\theta}_1)$ should never increase, and should converge to a steady value by the end of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some gradient descent settings\n",
    "iterations = 1500;\n",
    "alpha = 0.01;\n",
    "\n",
    "# Run the gradient descent \n",
    "def gradient_Descent(x,y,theta,alpha,num_iter):\n",
    "    m=len(y)\n",
    "    J_history = np.zeros([num_iter, 1])\n",
    "    sum=np.transpose(np.zeros([2,1]))\n",
    "    for i in range(num_iter):\n",
    "        for j in range(m):\n",
    "            sum=sum+ ((np.dot(x[j,:],(theta))-y[j])*x[j,:])\n",
    "        \n",
    "        sum=sum*(alpha/m)\n",
    "        theta=theta-np.transpose(sum)\n",
    "        J_history[i]=Compute_Cost(x,y,theta)\n",
    "    return(theta,J_history)\n",
    "\n",
    "    \n",
    "theta,J_history= gradient_Descent(x,y,theta,alpha,iterations) \n",
    "\n",
    "# print theta to screen\n",
    "np.set_printoptions(precision=4)\n",
    "print('Theta found by gradient descent:\\n')\n",
    "print(\"theta_0 = {}, theta_1 = {}\\n\".format(*theta))\n",
    "\n",
    "print('Expected theta values (approx)\\n')\n",
    "#print(' -3.6303  1.1664\\n\\n');\n",
    "print('{} and {}\\n\\n'.format(-3.6303,1.1664))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.Fit the line with the help of the estimated parameters ${\\theta}_i$. <a class=\"anchor\" id=\"eighth-bullet\"></a>\n",
    "\n",
    "Will use your final parameters to plot the linear fit. The result should look something like as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Line_fit=np.matmul(x, theta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the linear fit\n",
    "plt.figure(num=0, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(x[:,1], y, 'ro', x[:,1], Line_fit, 'b', linewidth=0.5, markersize=4)\n",
    "plt.ylabel('Profit in $10,000s',fontsize=12)              # Set the y axis label\n",
    "plt.xlabel('Population of City in 10,000s',fontsize=12)   # Set the x axis label\n",
    "\n",
    "# Turn on the minor TICKS, which are required for the minor GRID\n",
    "plt.minorticks_on()\n",
    "\n",
    "# Customize the major grid\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='black')\n",
    "# Customize the minor grid\n",
    "\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
    "plt.legend([\"raining data\", \"Linear Regression\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the final values for ${\\theta}_0 ,{\\theta}_1 $ will also be used to make predictions on profits in areas of 35,000 and 70,000 people. Note the way that the following lines in uses matrix multiplication, rather than explicit summation or looping, to calculate the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values for population sizes of 35,000 and 70,000\n",
    "predict1 =np.matmul ([1, 3.5] ,theta)\n",
    "print('For population = 35,000, we predict a profit of %f\\n' % (predict1*10000))\n",
    "predict2 =np.matmul ([1, 7] ,theta)\n",
    "print('For population = 70,000, we predict a profit of %f\\n' % (predict2*10000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.Visualizing $J({\\theta}_0,{\\theta}_1)$ vs ${\\theta}_0,{\\theta}_1$.<a class=\"anchor\" id=\"ninth-bullet\"></a>\n",
    "\n",
    "To understand the cost function $J({\\theta}_0,{\\theta}_1)$.better, we will now plot the cost over a 2-dimensional grid of ${\\theta}_0$ and ${\\theta}_1$ values. you should understand how the below code have written to create the 2-D plot and contour plot.\n",
    "\n",
    "Think about this:\n",
    "1. Why here contour plot is used ?\n",
    "\n",
    "**1. Surface plot**:\n",
    "\n",
    "In the next step there is code set up to calculate $J({\\theta}_0,{\\theta}_1)$ over a grid of values using the `compute_Cost` function that we had written above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Grid over which we will calculate J\n",
    "theta0_vals = np.linspace(-10, 10, 100)\n",
    "theta1_vals = np.linspace(-1, 4, 100)\n",
    "\n",
    "# initialize J_vals to a matrix of 0's\n",
    "J_vals = np.zeros([len(theta0_vals), len(theta1_vals)])\n",
    "for i in range(len(theta0_vals)):\n",
    "    for j in range(len(theta1_vals)):\n",
    "        t = [theta0_vals[i], theta1_vals[j]]\n",
    "        J_vals[[i],[j]]=Compute_Cost(x,y,t)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure(figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(theta0_vals, theta1_vals, np.transpose(J_vals), 50, rstride=1, cstride=1,\n",
    "                cmap=\"viridis\", edgecolor=\"none\")\n",
    "ax.set_xlabel(r'$\\theta_0$', fontsize=20)\n",
    "ax.set_ylabel(r'$\\theta_1$', fontsize=20)\n",
    "ax.set_zlabel(r'$J({\\theta}_0,{\\theta}_1)$', fontsize=20)\n",
    "\n",
    "#ax.view_init(12.5, 47)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Contour plot** \n",
    "\n",
    "The answer for the above question,Why here we used contour plot is very simple basically we want to simplified version of the above 3-D plot of $J({\\theta}_0,{\\theta}_1)$ vs ${\\theta}_0,{\\theta}_1$. So we go for 2-D version of the above surface plot which is **Contour plot**  and that plot is imagine as if you remove the `ax.view_init(12.5, 47)`from above code and see the resultant plot from top view it would be number of concentric circles.Where center is of the plot is our local optimum point. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the contour plot. \n",
    "fig = plt.figure(figsize=(8,6))\n",
    "left, bottom, width, height = 0.1, 0.1, 0.8, 0.8\n",
    "ax = fig.add_axes([left, bottom, width, height]) \n",
    "\n",
    "\n",
    "cp = plt.contourf(theta0_vals,theta1_vals ,np.transpose(J_vals),np.logspace(-2.0, 3.0, num=20))\n",
    "plt.colorbar(cp)\n",
    "plt.plot(theta[0],theta[1],'w*')\n",
    "ax.set_title('Contour Plot')\n",
    "ax.set_xlabel(r'$\\theta_0$', fontsize=20)\n",
    "ax.set_ylabel(r'$\\theta_1$', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "left, bottom, width, height = 0.1, 0.1, 0.8, 0.8\n",
    "ax = fig.add_axes([left, bottom, width, height]) \n",
    "\n",
    "cp = ax.contour(theta0_vals,theta1_vals ,np.transpose(J_vals))#level=np.logspace(-2.0, 3.0, num=20))\n",
    "#ax.clabel(cp, inline=True, \n",
    "         # fontsize=10)\n",
    "plt.plot(theta[0],theta[1],'r*')\n",
    "\n",
    "ax.set_title('Contour Plot')\n",
    "ax.set_xlabel(r'$\\theta_0$', fontsize=20)\n",
    "ax.set_ylabel(r'$\\theta_1$', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_extra=np.zeros([2,1])  # initialize fitting parameters again \n",
    "theta_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 1500;\n",
    "alpha = 0.01;\n",
    "theta1=np.zeros([1500, 1])\n",
    "theta2=np.zeros([1500, 1])\n",
    "# Run the gradient descent \n",
    "def gradient_Descent(x,y,theta_extra,alpha,num_iter):\n",
    "    m=len(y)\n",
    "    J_history = np.zeros([num_iter, 1])\n",
    "    sum=np.transpose(np.zeros([2,1]))\n",
    "    for i in range(num_iter):\n",
    "        for j in range(m):\n",
    "            sum=sum+ ((np.dot(x[j,:],(theta_extra))-y[j])*x[j,:])\n",
    "        \n",
    "        sum=sum*(alpha/m)\n",
    "        theta_extra=theta_extra-np.transpose(sum)\n",
    "        for k in range(1):\n",
    "            theta1[i]=theta_extra[k]\n",
    "            theta2[i]=theta_extra[k+1]\n",
    "            #print(theta1[i],theta2[i])\n",
    "\n",
    "gradient_Descent(x,y,theta_extra,alpha,iterations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "def Contour_Plot(T):\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    left, bottom, width, height = 0.1, 0.1, 0.8, 0.8\n",
    "    ax = fig.add_axes([left, bottom, width, height]) \n",
    "\n",
    "\n",
    "    cp = plt.contourf(theta0_vals,theta1_vals ,np.transpose(J_vals),np.logspace(-2.0, 3.0, num=20))\n",
    "    plt.colorbar(cp)\n",
    "    plt.plot(theta1[T],theta2[T],'w*')\n",
    "    ax.set_title('Contour Plot')\n",
    "    ax.set_xlabel(r'$\\theta_0$', fontsize=20)\n",
    "    ax.set_ylabel(r'$\\theta_1$', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "#Contour_Plot(theta1[100],theta2[100])\n",
    "\n",
    "ipyw.interact(\n",
    "    Contour_Plot,\n",
    "    T=ipyw.IntSlider(min=0, \n",
    "                     max=1499, \n",
    "                     step=1, \n",
    "                     value=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot you can vary the values of the Parameters ${\\theta}_0 ,{\\theta}_1$ by varying the value of `T` from the slider.You can see as you change the `T` which in turn change the Parameters ${\\theta}_0 ,{\\theta}_1$ by changing the `theta1[]` and `theta2[]` and see the **White star** moving towards an local optimum value.\n",
    "\n",
    "Below is the plot of the $J({\\theta})$ vs Number of Iterations,Which is evidence that as the iterations increases the value of the  $J({\\theta})$ decreases and at one point it get saturate to the value `4.834`. We will talk more about this in multiple linear regression model or linear Regression with multiple variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(J_history[1499])\n",
    "plt.figure(num=0, figsize=(6, 4), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(np.linspace(0,1499,1500),J_history,'r',linewidth=1.5)\n",
    "plt.ylabel(r'$J({\\theta})$',fontsize=12)\n",
    "plt.xlabel('Number of Iterations',fontsize=12)\n",
    "\n",
    "# Turn on the minor TICKS, which are required for the minor GRID\n",
    "plt.minorticks_on()\n",
    "\n",
    "# Customize the major grid\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='black')\n",
    "# Customize the minor grid\n",
    "\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
    "plt.legend([r'$J({\\theta})$ vs Iterations'], loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.Linear Regression using scikit-learn.<a class=\"anchor\" id=\"tenth-bullet\"></a>\n",
    "\n",
    "This is the linear regression model implemented using `scikit-learn` library.The document for `scikit-learn` is [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from IPython.display import display, Math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Data Visualization <a class=\"anchor\" id=\"9.1\"></a>\n",
    "\n",
    "Here we will use `seaborn.jointplot` to visualize the data.The document link is [here](https://seaborn.pydata.org/generated/seaborn.jointplot.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "sns.jointplot(x=df['Population'], y=df['Profit'], data=df, kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Splitting data into test and train <a class=\"anchor\" id=\"9.2\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df['Population'], df['Profit'], test_size=0.2, random_state=42)\n",
    "x_train=x_train.tolist()\n",
    "x_test=x_test.tolist()\n",
    "y_train=y_train.tolist()\n",
    "y_test=y_test.tolist()\n",
    "x_train = np.reshape(x_train, (-1,1))\n",
    "x_test = np.reshape(x_test, (-1,1))\n",
    "y_train = np.reshape(y_train, (-1,1))\n",
    "y_test = np.reshape(y_test, (-1,1))\n",
    "\n",
    "#\n",
    "print('Train - Predictors shape', x_train.shape)\n",
    "print('Test - Predictors shape', x_test.shape)\n",
    "print('Train - Target shape', y_train.shape)\n",
    "print('Test - Target shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = linear_model.LinearRegression()\n",
    "#Fit method is used for fitting your training data into the model\n",
    "cls.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = cls.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters used for the model \n",
    "cls.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Co-efficient of linear regression',cls.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intercept of linear regression model',cls.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Square Error', metrics.mean_squared_error(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Math(r'Model R^2 Square \\ value =\\ {}'.format( metrics.r2_score(y_test, prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Regression line on test set\n",
    "plt.scatter(x_test, y_test)\n",
    "plt.plot(x_test, prediction, color='red', linewidth=1)\n",
    "plt.xlabel('Population')\n",
    "plt.ylabel('Profit')\n",
    "plt.title('Linear Regression')\n",
    "plt.grid(which='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.Linear Regression from Statistical point of view and Least Square.  <a class=\"anchor\" id=\"eleventh-bullet\"></a>\n",
    "\n",
    "The statistical point of view is related to the values of the `mean square error` and the ${R^2}$ value we estimated last using the `Scikit-learn`. We will understand this concept with simple example and visualization.lets start what is **least square** which we had little idea as If you carefully see the cost function $J({\\theta})$ which we are trying to minimize to find ${\\theta}$ is actually an **error value** or **average square error** which we want as least value,hence called **Least Square error**.Last square is most popular method to find out the fitting parameter ${\\theta}s$ analytically. One simple linear algebra method or equation will help you to find out all the theta values.\n",
    "\n",
    "$$\n",
    "{\\theta}=(X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "Where,\n",
    "$X$ is Matrix of feature vectors and $y$ is target vector. As we are dealing with simple linear regression with one variable or one feature $x_1$ and also consider $x_0=1$, consider we have $m$ examples for our single feature $x_1$.\n",
    "\n",
    "$$\n",
    "X=\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_0 & x^{(1)}_1 \\\\\n",
    "x^{(2)}_0 & x^{(2)}_1 \\\\\n",
    "x^{(3)}_0 & x^{(3)}_1 \\\\\n",
    "\\vdots    & \\vdots    \\\\\n",
    "x^{(m)}_0 & x^{(m)}_1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X=\n",
    "\\begin{bmatrix}\n",
    "1 \\ & x^{(1)}_1 \\\\\n",
    "1 \\ & x^{(2)}_1 \\\\\n",
    "1 \\ & x^{(3)}_1 \\\\\n",
    "\\vdots & \\vdots  \\\\\n",
    "1 \\ & x^{(m)}_1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "$$\n",
    "y=\n",
    "\\begin{bmatrix}\n",
    "y^{(1)} \\\\\n",
    "y^{(2)} \\\\\n",
    "y^{(3)} \\\\\n",
    "\\vdots\\\\\n",
    "y^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So now you might think from where we arrive this expression \n",
    "$$\n",
    "{\\theta}=(X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "for that you have read the theory of [Linear Algebra](http://facultymember.iaukhsh.ac.ir/images/Uploaded_files/[Strang_G.]_Linear_algebra_and_its_applications(4)[5881001].PDF) under chapter  **3 Orthogonality - 3.3 Projections and Least Squares**.I will try add the mathematics behind it as update this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to understand the concept of the $R^2$,consider the example that we have data of the bunch of the mouse for its weight and size.We want to predict the size of the mouse given the weight of the mouse.\n",
    "![data](SLR00.png)\n",
    "\n",
    "As shown below image we fit the line using **least square** or by finding optimum parameter ${\\theta}$ to minimize the cost $J(\\theta)$. \n",
    "![data](SLR.png)\n",
    "\n",
    "As shown below fig the y-axis intercept and the slope are the two parameters which least square estimate.\n",
    "![data](SLR1.png)\n",
    "\n",
    "Since the slope is not zero knowing the mouse weight will help us to guess the mouse size.But how good that guess will be.Here $R^2$ comes in to the picture. calculating the $R^2$ will be the first step in determining how good that guess will be.   \n",
    "\n",
    "![data](SLR0.png)\n",
    "\n",
    "Now simplicity do not consider the weight of the mouse and shift all the data points on the y-axis by considering only the size of mouse.\n",
    "\n",
    "![data](SLR2.png)\n",
    "\n",
    "Now we will draw the black line which has all x-coordinate zero and will pass from the y-axis at the point of the average size mouse size and parallel to x-axis.just like least square we measure the distance from the mean value or average value or from the black line to the data points ,square it and add it together as shown below figures.\n",
    "\n",
    "![data](SLR3.png)\n",
    "![data](SLR4.png)\n",
    "\n",
    "Dividing that value by $n$,the number of data points we are estimating the variation around the mean.i.e. average sum of square per mouse as:\n",
    "\n",
    "$$\n",
    "var(mean)= \\frac{Data- mean}{n}\n",
    "$$\n",
    "\n",
    "**NOTE**:This variation is for the mouse size by not considering the mouse weight.\n",
    "\n",
    "Now suppose we fit the line using least square method as shown below and estimate the variance which is our least square error or average square error.\n",
    "\n",
    "![data](SLR5.png)\n",
    "\n",
    "\n",
    "$$\n",
    "var(fit)= \\frac{Data- LineFit}{n}\n",
    "$$\n",
    "\n",
    "\n",
    "![data](SLR6.png)\n",
    "\n",
    "Above figure is evidence that the variation is less for **least square fit is less** as compare to normal average line fit.We can say that variation can change or reduce as we consider the mouse weight.In other words heavier moue are bigger in size and vice-versa. **$R^2$ tells us how much variation in the mouse size can be explain by considering the weight of the mouse.** Thus formula for $R^2$ is \n",
    "\n",
    "$$\n",
    "R^2=\\frac{var(mean)-var(fit)}{var(mean)}\n",
    "$$\n",
    "![data](SLR7.png)\n",
    "\n",
    "Now let's say we have variance along the mean line is $var(mean)=11.1$. and variance along the fit line is $var(fit)=4.4$. then \n",
    "\n",
    "$$\n",
    "R^2=\\frac{11.1-4.4}{11.1}=0.6=60\\%\n",
    "$$\n",
    "\n",
    "This means there is $60\\%$ reduction in variance when we take mouse weight into account.Alternatively we can say that mouse weight **explains the $60\\%$ variation in the mouse size**.\n",
    "\n",
    "Now consider following example,Where my fitted line is perfectly fit i.e we have zero least square error or variance along the fitted line is zero.which means $R^2=1=100\\%$ \n",
    "![data](SLR8.png)\n",
    "![data](SLR9.png)\n",
    "\n",
    "$R^2=1=100\\%$ means the mouse weight explain 100% of the variation in the mouse size.\n",
    "\n",
    "one last example as shown below,Where the fitted line by least square is same as mean fit line.so both $var(mean)$ and $var(fit)$ are same value. so the $R^=0=0\\%$ \n",
    "![data](SLR10.png)\n",
    "![data](SLR11.png)\n",
    "\n",
    "So now $R^=0=0\\%$  the mouse weight doesn't explain the variation in the mouse size. As both heavier mouse and lighter mouse the size is varying equally likely. \n",
    "\n",
    "To understand more you can see this video lecture [StatQuest with Josh Starmer](https://www.youtube.com/watch?v=nk2CQITm_eo&t=312s).\n",
    "\n",
    "\n",
    "Now in our Profit prediction example we found out the $R^2=0.50$ value by using `sklearn.metrics` on the **Prediction** and **y_test**. This value $R^2=0.5$ tells us **how much the population explain the variation along the profit**.So,we try to improve this value by fitting best line. The $Mean Square Error \\ MSE= 15.709362447765187 $ is the least square error,or cost value $J({\\theta})$ after fitting the optimal line,estimated on the **Prediction** and **y_test**.Again the best fitted line will have less error. Next module we will implement Linear Regression using normal equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.Linear Regression using normal equations.<a class=\"anchor\" id=\"11\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function estiamte optimum values for parametrs using leas sqaure.\n",
    "\n",
    "def theta_calc_least_sqaure(x_train, y_train):\n",
    "    #Initializing all variables\n",
    "    n_data = x_train.shape[0]\n",
    "    bias = np.ones((n_data,1))\n",
    "    x_train_b = np.append(bias, x_train, axis=1)\n",
    "    #\n",
    "    value_1 = np.linalg.inv(np.dot(x_train_b.T,x_train_b))\n",
    "    value_2 = np.dot(value_1, x_train_b.T)\n",
    "    theta = np.dot(value_2,y_train)\n",
    "    #\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function predict the value on the test set uisng fitted parameters.\n",
    "def predict_func(slope,intercept,x_test):\n",
    "    pred = []\n",
    "    n_data = x_test.shape[0]\n",
    "    for i in range(n_data):\n",
    "        pred.append((slope * x_test[i]) + intercept)\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function estimate the MSE by using predicted value an actual value on test set.\n",
    "def MSE_calc(prediction, y_test):\n",
    "    #\n",
    "    total_data = len(prediction)\n",
    "    error = 0\n",
    "    error = (np.sum((prediction - y_test)**2))/total_data\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rsq(prediction, y_test):\n",
    "    total_data = len(prediction)\n",
    "    #Average of y_test set\n",
    "    y_avg = np.sum(y_test)/total_data\n",
    "    #total sum of square error \n",
    "    tot_err = np.sum((y_test-y_avg)**2)\n",
    "    # variance for y_test_set or var(mean)\n",
    "    var_tot_err=tot_err/total_data\n",
    "    #total sum of squared error or least square error\n",
    "    least_square_err = np.sum((y_test-prediction)**2)\n",
    "    # var(fit)\n",
    "    var_least_square_err=least_square_err/total_data\n",
    "    #\n",
    "    r2 = (var_tot_err-var_least_square_err)/var_tot_err\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding optimal theta value using normal equations\n",
    "theta = theta_calc_least_sqaure(x_train, y_train)\n",
    "intercept = theta[0]\n",
    "slope = theta[1]\n",
    "print('Intercept of the model', intercept)\n",
    "print('Slope of the model', slope)\n",
    "#Prediction calculation\n",
    "prediction = predict_func(slope, intercept, x_test)\n",
    "#MSE calculation\n",
    "error =  MSE_calc(prediction, y_test)\n",
    "print('Mean squared error of the model', error)\n",
    "#R-square calculation\n",
    "r2_val = Rsq(prediction, y_test)\n",
    "#print('R squared value', r2_val)\n",
    "display(Math(r'Model R^2 Square \\ value =\\ {}'.format(r2_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are same as we estimated using `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## References. <a class=\"anchor\" id=\"twelfth-bullet\"></a>\n",
    " \n",
    "   * [1] [Coursera ML course by Andrew Ng.](https://www.youtube.com/watch?v=PPLop4L2eGk&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=1)\n",
    "   \n",
    "   * [2] [Bishop - Pattern Recognition And Machine Learning - Springer  2006](https://www.academia.edu/17851990/Bishop_Pattern_Recognition_and_Machine_Learning)\n",
    "   \n",
    "   * [3] [towards Data science](https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86)\n",
    "   * [4] [Linear regression with one variable](https://www.internalpointers.com/post/linear-regression-one-variable)\n",
    "   * [5] [Linear Algebra by Gilbert Strang](http://facultymember.iaukhsh.ac.ir/images/Uploaded_files/[Strang_G.]_Linear_algebra_and_its_applications(4)[5881001].PDF))\n",
    "   * [6] [StatQuest with Josh Starmer.](https://www.youtube.com/watch?v=nk2CQITm_eo&t=312s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
